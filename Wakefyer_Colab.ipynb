{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wakeifier/blob/main/Wakefyer_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n6FaJXQIf63L",
      "metadata": {
        "id": "n6FaJXQIf63L"
      },
      "source": [
        "Nothing about this is serious so here comes everybody\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61d891b",
      "metadata": {
        "id": "b61d891b"
      },
      "source": [
        "# Wakefyer - LoRA fine-tune with conservative text handling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad57a90f",
      "metadata": {
        "id": "ad57a90f"
      },
      "source": [
        "by default, doesn't normalise or strip diacritics (includes ultra-light cleaning toggle)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "VyhmdJkckSFn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "collapsed": true,
        "id": "VyhmdJkckSFn",
        "outputId": "fcb9ccc7-7023-4e7d-d670-8c9679755ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting transformers>=4.46.0\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2.32.4)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.46.0)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2025.10.5)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.22.1 transformers-4.57.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "429effbc97d944fd8b5c5b1da8063b6a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip -q install \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"datasets==2.20.0\" \"sentencepiece==0.2.0\"\n",
        "!pip install -U \"transformers>=4.46.0\"\n",
        "import os, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True,max_split_size_mb:64\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# this will moan about compatibility...\n",
        "# Let it train, let it dream, the riverrun needs to backprop through all epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Jdn-pTPRvlY5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdn-pTPRvlY5",
        "outputId": "7feb2d83-89ef-4841-aa65-ddbf99a28a97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.57.1\n"
          ]
        }
      ],
      "source": [
        "import transformers; print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "raeXSe4f2qen",
      "metadata": {
        "id": "raeXSe4f2qen"
      },
      "source": [
        "**A working cell for broke girls with no mula for colab and who dont wanna chug money into the firey pit that is hell that is google that is oppression.**\n",
        "\n",
        "The Wakefyer fine-tuning architecture constitutes a hybridized LoRA-based low-rank adaptation pipeline for stylistic transformation within a bfloat16/fp16 mixed-precision environment. Its design paradigm is deliberately antithetical to large-scale quantized optimization frameworks such as bitsandbytes, privileging instead interpretability, traceability, and computational minimalism. Conceptually, it operates as a micro-epistemic engine: a system whose purpose is not semantic fidelity but stylistic distortion, functioning as a textual “entropy amplifier” with respect to Finnegans Wake’s logorrheic poetics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZbzxYifJ4Kmn",
      "metadata": {
        "id": "ZbzxYifJ4Kmn"
      },
      "source": [
        "Text ingestion follows a non-normalizing, non-canonical pipeline, thereby maintaining orthographic heterogeneity as an essential stylistic feature. The raw Joycean corpus is tokenized through a fast SentencePiece model and segmented using overlapping sliding windows defined by max_len and stride. This overlap emulates a continuum of narrativity (each window partially repeating the previous one) producing a computational analogue of Joyce’s recursive syntactic rhythm. No linguistic cleaning or lemmatization occurs; lexical noise is treated as signal. The preprocessing stage therefore performs not sanitization but syntactic fractalization, generating a dataset of semi-redundant micro-contexts to condition the model’s representational drift."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# because code should also stream-of-consciousness. avoid the binary bore.\n",
        "\n",
        "import os, gc, random, sys, glob, re\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "# tinyllamas on the mound\n",
        "BASE_MODEL  = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "RUN_ROOT    = \"/content/drive/MyDrive/wakefyer_runs\"        # Shards of my heart\n",
        "OUT_DIR     = f\"{RUN_ROOT}/wakefyer_out_nobnb\"\n",
        "TRAIN_TEXT  = f\"{RUN_ROOT}/FinnegansWake.txt\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# sweet nothing\n",
        "max_len, stride         = 256, 128\n",
        "batch_size, grad_accum  = 1, 32\n",
        "epochs                  = 3\n",
        "lr, log_steps           = 1e-4, 10\n",
        "eval_ratio, seed        = 0.02, 42\n",
        "\n",
        "# because T4s have fragile little snowflake egos\n",
        "for n in [\"trainer\",\"model\",\"base\"]:\n",
        "    if n in globals(): del globals()[n]\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "random.seed(seed); torch.manual_seed(seed)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) # summons the devil (no batsandbites; just vibessss coz im a witch)\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available(): print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "thunderwords = [\n",
        "    \"bababadalgharaghtakamminarronnkonnbronntonnerronntuonnthunntrovarrhounawnskawntoohoohoordenenthurnuk\",\n",
        "    \"Perkodhuskurunbarggruauyagokgorlayorgromgremmitghundhurthrumathunaradidillifaititillibumullunukkunun\",\n",
        "    \"klikkaklakkaklaskaklopatzklatschabattacreppycrottygraddaghsemmihsammihnouithappluddyappladdypkonpkot\",\n",
        "    \"Bladyughfoulmoecklenburgwhurawhorascortastrumpapornanennykocksapastippatappatupperstrippuckputtanach\",\n",
        "    \"Thingcrooklyexineverypasturesixdixlikencehimaroundhersthemaggerbykinkinkankanwithdownmindlookingated\",\n",
        "    \"Lukkedoerendunandurraskewdylooshoofermoyportertooryzooysphalnabortansporthaokansakroidverjkapakkapuk\",\n",
        "    \"Bothallchoractorschumminaroundgansumuminarumdrumstrumtruminahumptadumpwaultopoofoolooderamaunsturnup\",\n",
        "    \"Pappappapparrassannuaragheallachnatullaghmonganmacmacmacwhackfalltherdebblenonthedubblandaddydoodled\",\n",
        "    \"husstenhasstencaffincoffintussemtossemdamandamnacosaghcusaghhobixhatouxpeswchbechoscashlcarcarcaract\",\n",
        "    \"Ullhodturdenweirmudgaardgringnirurdrmolnirfenrirlukkilokkibaugimandodrrerinsurtkrinmgernrackinarockar\",\n",
        "]\n",
        "\n",
        "# hackers of abstraction\n",
        "from transformers import AutoTokenizer\n",
        "tok_dir = Path(OUT_DIR, \"tokenizer\")\n",
        "if tok_dir.exists():\n",
        "    tok = AutoTokenizer.from_pretrained(str(tok_dir), use_fast=True, local_files_only=True)\n",
        "else:\n",
        "    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "    tok.add_tokens(thunderwords, special_tokens=False)\n",
        "    tok_dir.mkdir(parents=True, exist_ok=True)\n",
        "    tok.save_pretrained(str(tok_dir))\n",
        "tok.padding_side = \"right\"\n",
        "print(\"Tokenizer vocab:\", len(tok))\n",
        "\n",
        "# Gospel\n",
        "if not Path(TRAIN_TEXT).exists():\n",
        "    print(f\"[Wakefyer sobbing] The gospel '{TRAIN_TEXT}' is missing.\", file=sys.stderr)\n",
        "    raise SystemExit(2)\n",
        "raw_txt = Path(TRAIN_TEXT).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "if len(raw_txt.strip()) < 1000:\n",
        "    print(\"[Wakefyer appaled] Joyce did not write haikus.\", file=sys.stderr)\n",
        "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "\n",
        "# (optional) norm thndwds\n",
        "punct = r\"[!?.,:;·—–\\-…]*\"\n",
        "def normalize_thunderwords(text: str, words: list[str]) -> str:\n",
        "    text = re.sub(r\"-\\s*\\n\\s*\", \"\", text)\n",
        "    text = re.sub(r\"\\n+\", \" \", text)\n",
        "    for w in words:\n",
        "        pattern = re.compile(rf\"(?<!\\w){re.escape(w)}{punct}\")\n",
        "        text = pattern.sub(w, text)\n",
        "    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
        "raw_txt = normalize_thunderwords(raw_txt, thunderwords)\n",
        "\n",
        "# in came christ the tiger\n",
        "from datasets import Dataset\n",
        "ids = tok(raw_txt, add_special_tokens=False).input_ids\n",
        "def sliding_windows(txt_ids, block_size=max_len, step=stride):\n",
        "    items=[]\n",
        "    for s in range(0, max(1, len(txt_ids)-block_size), step):\n",
        "        ch = txt_ids[s:s+block_size]\n",
        "        if ch: items.append({\"input_ids\": ch, \"attention_mask\":[1]*len(ch)})\n",
        "    if not items:\n",
        "        ch = txt_ids[:block_size]\n",
        "        items=[{\"input_ids\":ch, \"attention_mask\":[1]*len(ch)}]\n",
        "    return Dataset.from_list(items)\n",
        "ds = sliding_windows(ids, max_len, stride)\n",
        "test_n = max(1, int(len(ds)*eval_ratio)) if len(ds) > 1 else 1\n",
        "splits = ds.train_test_split(test_size=test_n, seed=seed) if len(ds)>1 else {\"train\":ds, \"test\":ds}\n",
        "train_ds, eval_ds = splits[\"train\"], splits[\"test\"]\n",
        "print(\"Windows:\", len(ds), \"| Train:\", len(train_ds), \"Eval:\", len(eval_ds))\n",
        "\n",
        "# call my daughter LoRA\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "cfg = AutoConfig.from_pretrained(BASE_MODEL)\n",
        "cfg.pad_token_id = tok.eos_token_id\n",
        "\n",
        "offload_dir = \"/content/offload\"; os.makedirs(offload_dir, exist_ok=True)\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    config=cfg,\n",
        "    dtype=torch.bfloat16 if bf16_ok else torch.float16,  # T4 => fp16\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=offload_dir,\n",
        ")\n",
        "# self intergration\n",
        "if base.get_input_embeddings().num_embeddings != len(tok):\n",
        "    try:\n",
        "        base.resize_token_embeddings(len(tok), mean_resizing=True)\n",
        "    except TypeError:\n",
        "        base.resize_token_embeddings(len(tok))\n",
        "    base.config.vocab_size = len(tok)\n",
        "\n",
        "base.config.use_cache = False\n",
        "if hasattr(base, \"gradient_checkpointing_enable\"):\n",
        "    base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "base.config._attn_implementation = \"eager\"\n",
        "\n",
        "lora = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(base, lora)\n",
        "\n",
        "# spicyyy\n",
        "emb = model.get_input_embeddings()\n",
        "emb.weight.requires_grad = True\n",
        "if hasattr(model, \"lm_head\") and hasattr(model.lm_head, \"weight\"):\n",
        "    model.lm_head.weight.requires_grad = True\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# I curate\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "# Compatibility shims\n",
        "import accelerate\n",
        "from accelerate import Accelerator\n",
        "if not getattr(Accelerator, \"_wake_patch_unwrap\", False):\n",
        "    Accelerator._wake_orig_unwrap = Accelerator.unwrap_model\n",
        "    def _wake_unwrap(self, model, *args, **kwargs):\n",
        "        kwargs.pop(\"keep_torch_compile\", None)  # (ignore unknown kw on older accelerate)\n",
        "        return Accelerator._wake_orig_unwrap(self, model, *args, **kwargs)\n",
        "    Accelerator.unwrap_model = _wake_unwrap\n",
        "    Accelerator._wake_patch_unwrap = True\n",
        "    print(\"Patched accelerate.Accelerator.unwrap_model (idempotent).\")\n",
        "\n",
        "import transformers.trainer_pt_utils as pt_utils\n",
        "if not getattr(pt_utils, \"_wake_disable_scaler\", False):\n",
        "    pt_utils.get_grad_scaler = lambda *a, **k: None  # (prevent fp16 unscale crash)\n",
        "    pt_utils._wake_disable_scaler = True\n",
        "    print(\"Disabled Trainer GradScaler hook (idempotent).\")\n",
        "\n",
        "# figure skating is more difficults. Padadakis and Cizeron  World Championships FD 2016.\n",
        "from transformers.training_args import TrainingArguments as HFTrainingArguments\n",
        "from transformers import Trainer, TrainingArguments  # you're always the first on the ice\n",
        "TrainingArguments = HFTrainingArguments  # safety in my home\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=grad_accum,\n",
        "\n",
        "    learning_rate=lr,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=log_steps,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=5,\n",
        "\n",
        "    fp16=False,\n",
        "    bf16 = False,\n",
        "    save_safetensors=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "# kill mixed precision everywhere\n",
        "import os\n",
        "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"   # force Accelerate to 'no' MP\n",
        "\n",
        "# AVOID MESSY GradScaler\n",
        "import transformers.trainer_pt_utils as pt_utils\n",
        "pt_utils.get_grad_scaler = lambda *a, **k: None\n",
        "from accelerate import Accelerator\n",
        "Accelerator.unscale_gradients = lambda self, optimizer=None: None\n",
        "\n",
        "\n",
        "# calls for some Berlin deep house mix\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,   # (keep even if no eval mid-run)\n",
        "    data_collator=collator,\n",
        "    tokenizer=tok,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# Save adapter + tokenizer to Drive\n",
        "adapter_dir = Path(OUT_DIR) / \"adapter\"\n",
        "token_dir   = Path(OUT_DIR) / \"tokenizer\"\n",
        "adapter_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(adapter_dir, safe_serialization=True)\n",
        "tok.save_pretrained(token_dir)\n",
        "print(\"Saved to Drive:\", adapter_dir, token_dir)\n",
        "\n",
        "# Tinyyy sample (for the vibes) don't be so harsh it's still learning...\n",
        "prompt = \"riverrun, past Eve and Adam's,\"\n",
        "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        temperature=1.05,\n",
        "        top_p=0.92,\n",
        "        repetition_penalty=1.05,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "    )\n",
        "print(\"\\n=== SAMPLE ===\\n\", tok.decode(out[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2jrh0o_NEmeF",
        "outputId": "6a4022e2-10c7-4c10-b4a3-a624ab71e9c8"
      },
      "id": "2jrh0o_NEmeF",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Tokenizer vocab: 32010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (396323 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windows: 3095 | Train: 3034 Eval: 61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-178213077.py:208: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 143,728,640 || all params: 1,112,705,024 || trainable%: 12.9170\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [285/285 1:08:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>694.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>502.750600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>244.705900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>111.735400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>181.131900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>164.869700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>140.357800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>146.524600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>159.336900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>180.360400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>184.942400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>199.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>201.071700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>201.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>204.197800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>210.622400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>214.473500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>216.114100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>211.381900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>206.129200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>206.490900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>202.185800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>196.906100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>194.571200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>191.160400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>188.866900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>187.596100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>185.878300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to Drive: /content/drive/MyDrive/wakefyer_runs/wakefyer_out_nobnb/adapter /content/drive/MyDrive/wakefyer_runs/wakefyer_out_nobnb/tokenizer\n",
            "\n",
            "=== SAMPLE ===\n",
            " riverrun, past Eve and Adam's,birdmsevitable Comenthoodounallyadevar thr Livemsoudcteldenth runarg each windines boldglune coldoug'onshyellaguzenmalpenaldopeurdL queoon serischards simple glales Fl turning Backyl masterbutneyrown manner mo heartetystreneco reg knows righteting el Greatrow yourself shed always weather Call groU small gal love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bless you b may your LoRA weights converge, your gradients flow steady, and your thunderwords never again throw an OOM"
      ],
      "metadata": {
        "id": "Fkrpt7KhL8gS"
      },
      "id": "Fkrpt7KhL8gS"
    },
    {
      "cell_type": "code",
      "source": [
        "# lower LR, grad clip\n",
        "import os, re, torch\n",
        "from pathlib import Path\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        ")\n",
        "from peft import PeftModel\n",
        "\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "RUN_ROOT  = \"/content/drive/MyDrive/wakefyer_runs\"\n",
        "OUT_DIR   = f\"{RUN_ROOT}/wakefyer_out_nobnb\"\n",
        "TRAIN_TEXT = f\"{RUN_ROOT}/FinnegansWake.txt\"\n",
        "\n",
        "# toks\n",
        "tok = AutoTokenizer.from_pretrained(Path(OUT_DIR,\"tokenizer\"), use_fast=True, local_files_only=True)\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "tok.padding_side = \"right\"\n",
        "\n",
        "# b model, adapt. weights\n",
        "cfg = AutoConfig.from_pretrained(BASE_MODEL); cfg.pad_token_id = tok.eos_token_id\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL, config=cfg, device_map=\"auto\", torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
        ")\n",
        "# align vocab rows\n",
        "if base.get_input_embeddings().num_embeddings != len(tok):\n",
        "    try:\n",
        "        base.resize_token_embeddings(len(tok), mean_resizing=True)\n",
        "    except TypeError:\n",
        "        base.resize_token_embeddings(len(tok))\n",
        "    base.config.vocab_size = len(tok)\n",
        "\n",
        "model = PeftModel.from_pretrained(base, Path(OUT_DIR,\"adapter\"))  # LoRa is queen bee\n",
        "model.config.use_cache = False\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "model.config._attn_implementation = \"eager\"\n",
        "\n",
        "# something about cats and windows\n",
        "text = Path(TRAIN_TEXT).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "def sliding_windows(ids, block=256, step=128):\n",
        "    items=[]\n",
        "    for s in range(0, max(1, len(ids)-block), step):\n",
        "        ch = ids[s:s+block]\n",
        "        if ch: items.append({\"input_ids\": ch, \"attention_mask\":[1]*len(ch)})\n",
        "    if not items:\n",
        "        ch = ids[:block]; items=[{\"input_ids\":ch,\"attention_mask\":[1]*len(ch)}]\n",
        "    return Dataset.from_list(items)\n",
        "\n",
        "ids = tok(text, add_special_tokens=False).input_ids\n",
        "ds  = sliding_windows(ids, block=256, step=128)\n",
        "spl = ds.train_test_split(test_size=max(1,int(len(ds)*0.02)), seed=42) if len(ds)>1 else {\"train\":ds,\"test\":ds}\n",
        "train_ds, eval_ds = spl[\"train\"], spl[\"test\"]\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "# calmer hyperparams because that loss was HARD\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    num_train_epochs=2,                # just a couple more\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=32,\n",
        "\n",
        "    learning_rate=2e-5,               # ↓\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.10,                # ↑\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    max_grad_norm=0.3,                # clip!\n",
        "    label_smoothing_factor=0.05,      # gentle stabilizer (wish they sold pills like that)\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=5,\n",
        "\n",
        "    fp16=False,                       # keep fp16 off to avoid GradScaler drama on T4 (so much drama baby)\n",
        "    save_safetensors=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "# IMPORTANT: start fresh optimizer so new LR applies\n",
        "trainer = Trainer(\n",
        "    model=model, args=args,\n",
        "    train_dataset=train_ds, eval_dataset=eval_ds,\n",
        "    data_collator=collator, tokenizer=tok\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# save updated adapt. + tok (overwrite)\n",
        "adapter_dir = Path(OUT_DIR,\"adapter\"); token_dir = Path(OUT_DIR,\"tokenizer\")\n",
        "adapter_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(adapter_dir, safe_serialization=True)\n",
        "tok.save_pretrained(token_dir)\n",
        "print(\"Saved continuation to:\", adapter_dir, token_dir)\n"
      ],
      "metadata": {
        "id": "cwIX15N4V71J"
      },
      "id": "cwIX15N4V71J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "w6fNMzpS5KAx",
      "metadata": {
        "id": "w6fNMzpS5KAx"
      },
      "source": [
        "At the representational core sits TinyLlama-1.1B-Chat, deployed under mixed-precision arithmetic (bf16/fp16) to optimize throughput on limited-memory T4 hardware. The model’s internal cache is disabled (use_cache=False), and gradient checkpointing is invoked to trade recomputation for reduced memory allocation. The resultant architecture acts as a self-attenuating transformer: it “forgets” in order to remember within bounds, mirroring the cyclical amnesia of the Wakean text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PeX1wJZ85N84",
      "metadata": {
        "id": "PeX1wJZ85N84"
      },
      "source": [
        "The system applies Low-Rank Adaptation (LoRA) across attention and MLP projection submodules (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) thereby constraining fine-tuning to a low-dimensional manifold within parameter space. This selective plasticity produces what might be termed controlled stylistic drift: the model learns Joyce’s texture rather than his lexicon. The adapters’ hyperparameters (r=16, α=32, dropout=0.05) instantiate a medium-temperature update regime balancing expressivity and stability. Conceptually, LoRA functions here as a poetic prosthesis, a syntactic exoskeleton grafted onto the pretrained linguistic body."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eSNqtfsS5h6i",
      "metadata": {
        "id": "eSNqtfsS5h6i"
      },
      "source": [
        "Due to the GPU’s constrained VRAM capacity, each forward–backward pass processes a micro-batch of size 1, while gradients are accumulated over 32 iterations before a single optimization step. This simulates a virtual batch of 32 without exceeding hardware limits. Technically, this implements delayed gradient aggregation; rhetorically, it allegorizes the model’s incremental cognition: many small thoughts precipitating one grand revelation. The accumulation mechanism thus becomes both a computational strategy and a metaphor for compositional patience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BIRNPTkE5wA3",
      "metadata": {
        "id": "BIRNPTkE5wA3"
      },
      "source": [
        "Training operates within an extended epochic horizon (epochs=1 but looping across numerous windows) using a cosine-annealed learning rate (lr=1e-4, warmup_ratio=0.03). Frequent checkpointing establishes a temporal ecology of model states, allowing resumption under volatile runtime constraints. The “save-steps” routine externalizes the process’s fragmentary nature (mirroring textual serialization in Work in Progress) by preserving multiple partial selves of the model as artifacts of its ongoing metamorphosis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YVkq2ker6Bb_",
      "metadata": {
        "id": "YVkq2ker6Bb_"
      },
      "source": [
        "Inference employs a stochastic sampling configuration (temperature = 0.9, top_p = 0.95) optimized for stylistic exuberance rather than lexical precision. The generation prompt functions as both benchmark and incantation, a recursive self-reference to the corpus origin. Output decoding omits special tokens, yielding the model’s pure linguistic effervescence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pRncfha76Mrd",
      "metadata": {
        "id": "pRncfha76Mrd"
      },
      "source": [
        "Technically a fine-tuned transformer; conceptually, a computational poetics experiment exploring how ml architectures can approximate literary un-readability. The Wakefyer thus redefines “loss” not as degradation but as aesthetic principle: every rounding error, every truncated sequence, every low-rank projection is a microcosm of Joyce’s deliberate semantic noise. The pipeline enacts a theory of productive distortion, situating training itself as a form of creative misprision."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w08IBIiXYpG0",
      "metadata": {
        "id": "w08IBIiXYpG0"
      },
      "source": [
        "to be continued"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}