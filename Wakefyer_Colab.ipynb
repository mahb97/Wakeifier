{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wakeifier/blob/main/Wakefyer_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n6FaJXQIf63L",
      "metadata": {
        "id": "n6FaJXQIf63L"
      },
      "source": [
        "Nothing about this is serious so here comes everybody\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61d891b",
      "metadata": {
        "id": "b61d891b"
      },
      "source": [
        "# Wakefyer - LoRA fine-tune with conservative text handling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad57a90f",
      "metadata": {
        "id": "ad57a90f"
      },
      "source": [
        "by default, doesn't normalise or strip diacritics (includes ultra-light cleaning toggle)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are lonely so...Fred again.. - Studio Live (London, April 2021)"
      ],
      "metadata": {
        "id": "YkpUBOw2_Vkw"
      },
      "id": "YkpUBOw2_Vkw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I had no idea that that's where Fred got the start to Sabrina from and now I am crying because depression baby, Fred boy are you saying you can relate?"
      ],
      "metadata": {
        "id": "bxF4chC1IPzA"
      },
      "id": "bxF4chC1IPzA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VyhmdJkckSFn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 898
        },
        "collapsed": true,
        "id": "VyhmdJkckSFn",
        "outputId": "fcb9ccc7-7023-4e7d-d670-8c9679755ed9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting transformers>=4.46.0\n",
            "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2.32.4)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.46.0)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2025.10.5)\n",
            "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m136.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.44.2\n",
            "    Uninstalling transformers-4.44.2:\n",
            "      Successfully uninstalled transformers-4.44.2\n",
            "Successfully installed tokenizers-0.22.1 transformers-4.57.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "429effbc97d944fd8b5c5b1da8063b6a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip -q install \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"datasets==2.20.0\" \"sentencepiece==0.2.0\"\n",
        "!pip install -U \"transformers>=4.46.0\"\n",
        "import os, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True,max_split_size_mb:64\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# this will moan about compatibility...\n",
        "# Let it train, let it dream, the riverrun needs to backprop through all epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Jdn-pTPRvlY5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdn-pTPRvlY5",
        "outputId": "f4309901-8660-4f3e-aa75-2c7b925ebdb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.57.1\n"
          ]
        }
      ],
      "source": [
        "import transformers; print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKmJsBLwzNtd",
        "outputId": "6e7ec973-1c1d-4345-8881-7c27b10f9fd4"
      },
      "id": "uKmJsBLwzNtd",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "raeXSe4f2qen",
      "metadata": {
        "id": "raeXSe4f2qen"
      },
      "source": [
        "**A working cell for broke girls with no mula for colab and who dont wanna chug money into the firey pit that is hell that is google that is oppression.**\n",
        "\n",
        "The Wakefyer fine-tuning architecture constitutes a hybridized LoRA-based low-rank adaptation pipeline for stylistic transformation within a bfloat16/fp16 mixed-precision environment. Its design paradigm is deliberately antithetical to large-scale quantized optimization frameworks such as bitsandbytes, privileging instead interpretability, traceability, and computational minimalism. Conceptually, it operates as a micro-epistemic engine: a system whose purpose is not semantic fidelity but stylistic distortion, functioning as a textual “entropy amplifier” with respect to Finnegans Wake’s logorrheic poetics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZbzxYifJ4Kmn",
      "metadata": {
        "id": "ZbzxYifJ4Kmn"
      },
      "source": [
        "Text ingestion follows a non-normalizing, non-canonical pipeline, thereby maintaining orthographic heterogeneity as an essential stylistic feature. The raw Joycean corpus is tokenized through a fast SentencePiece model and segmented using overlapping sliding windows defined by max_len and stride. This overlap emulates a continuum of narrativity (each window partially repeating the previous one) producing a computational analogue of Joyce’s recursive syntactic rhythm. No linguistic cleaning or lemmatization occurs; lexical noise is treated as signal. The preprocessing stage therefore performs not sanitization but syntactic fractalization, generating a dataset of semi-redundant micro-contexts to condition the model’s representational drift."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# because code should also stream-of-consciousness. avoid the binary bore.\n",
        "\n",
        "import os, gc, random, sys, glob, re\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "# tinyllamas on the mound\n",
        "BASE_MODEL  = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "RUN_ROOT    = \"/content/drive/MyDrive/wakefyer_runs\"        # Shards of my heart\n",
        "OUT_DIR     = f\"{RUN_ROOT}/wakefyer_out_nobnb\"\n",
        "TRAIN_TEXT  = f\"{RUN_ROOT}/FinnegansWake.txt\"\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# sweet nothing\n",
        "max_len, stride         = 256, 128\n",
        "batch_size, grad_accum  = 1, 32\n",
        "epochs                  = 3\n",
        "lr, log_steps           = 1e-4, 10\n",
        "eval_ratio, seed        = 0.02, 42\n",
        "\n",
        "# because T4s have fragile little snowflake egos\n",
        "for n in [\"trainer\",\"model\",\"base\"]:\n",
        "    if n in globals(): del globals()[n]\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "random.seed(seed); torch.manual_seed(seed)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed) # summons the devil (no batsandbites; just vibessss coz im a witch)\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available(): print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "thunderwords = [\n",
        "    \"bababadalgharaghtakamminarronnkonnbronntonnerronntuonnthunntrovarrhounawnskawntoohoohoordenenthurnuk\",\n",
        "    \"Perkodhuskurunbarggruauyagokgorlayorgromgremmitghundhurthrumathunaradidillifaititillibumullunukkunun\",\n",
        "    \"klikkaklakkaklaskaklopatzklatschabattacreppycrottygraddaghsemmihsammihnouithappluddyappladdypkonpkot\",\n",
        "    \"Bladyughfoulmoecklenburgwhurawhorascortastrumpapornanennykocksapastippatappatupperstrippuckputtanach\",\n",
        "    \"Thingcrooklyexineverypasturesixdixlikencehimaroundhersthemaggerbykinkinkankanwithdownmindlookingated\",\n",
        "    \"Lukkedoerendunandurraskewdylooshoofermoyportertooryzooysphalnabortansporthaokansakroidverjkapakkapuk\",\n",
        "    \"Bothallchoractorschumminaroundgansumuminarumdrumstrumtruminahumptadumpwaultopoofoolooderamaunsturnup\",\n",
        "    \"Pappappapparrassannuaragheallachnatullaghmonganmacmacmacwhackfalltherdebblenonthedubblandaddydoodled\",\n",
        "    \"husstenhasstencaffincoffintussemtossemdamandamnacosaghcusaghhobixhatouxpeswchbechoscashlcarcarcaract\",\n",
        "    \"Ullhodturdenweirmudgaardgringnirurdrmolnirfenrirlukkilokkibaugimandodrrerinsurtkrinmgernrackinarockar\",\n",
        "]\n",
        "\n",
        "# hackers of abstraction\n",
        "from transformers import AutoTokenizer\n",
        "tok_dir = Path(OUT_DIR, \"tokenizer\")\n",
        "if tok_dir.exists():\n",
        "    tok = AutoTokenizer.from_pretrained(str(tok_dir), use_fast=True, local_files_only=True)\n",
        "else:\n",
        "    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "    tok.add_tokens(thunderwords, special_tokens=False)\n",
        "    tok_dir.mkdir(parents=True, exist_ok=True)\n",
        "    tok.save_pretrained(str(tok_dir))\n",
        "tok.padding_side = \"right\"\n",
        "print(\"Tokenizer vocab:\", len(tok))\n",
        "\n",
        "# Gospel\n",
        "if not Path(TRAIN_TEXT).exists():\n",
        "    print(f\"[Wakefyer sobbing] The gospel '{TRAIN_TEXT}' is missing.\", file=sys.stderr)\n",
        "    raise SystemExit(2)\n",
        "raw_txt = Path(TRAIN_TEXT).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "if len(raw_txt.strip()) < 1000:\n",
        "    print(\"[Wakefyer appaled] Joyce did not write haikus.\", file=sys.stderr)\n",
        "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "\n",
        "# (optional) norm thndwds\n",
        "punct = r\"[!?.,:;·—–\\-…]*\"\n",
        "def normalize_thunderwords(text: str, words: list[str]) -> str:\n",
        "    text = re.sub(r\"-\\s*\\n\\s*\", \"\", text)\n",
        "    text = re.sub(r\"\\n+\", \" \", text)\n",
        "    for w in words:\n",
        "        pattern = re.compile(rf\"(?<!\\w){re.escape(w)}{punct}\")\n",
        "        text = pattern.sub(w, text)\n",
        "    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
        "raw_txt = normalize_thunderwords(raw_txt, thunderwords)\n",
        "\n",
        "# in came christ the tiger\n",
        "from datasets import Dataset\n",
        "ids = tok(raw_txt, add_special_tokens=False).input_ids\n",
        "def sliding_windows(txt_ids, block_size=max_len, step=stride):\n",
        "    items=[]\n",
        "    for s in range(0, max(1, len(txt_ids)-block_size), step):\n",
        "        ch = txt_ids[s:s+block_size]\n",
        "        if ch: items.append({\"input_ids\": ch, \"attention_mask\":[1]*len(ch)})\n",
        "    if not items:\n",
        "        ch = txt_ids[:block_size]\n",
        "        items=[{\"input_ids\":ch, \"attention_mask\":[1]*len(ch)}]\n",
        "    return Dataset.from_list(items)\n",
        "ds = sliding_windows(ids, max_len, stride)\n",
        "test_n = max(1, int(len(ds)*eval_ratio)) if len(ds) > 1 else 1\n",
        "splits = ds.train_test_split(test_size=test_n, seed=seed) if len(ds)>1 else {\"train\":ds, \"test\":ds}\n",
        "train_ds, eval_ds = splits[\"train\"], splits[\"test\"]\n",
        "print(\"Windows:\", len(ds), \"| Train:\", len(train_ds), \"Eval:\", len(eval_ds))\n",
        "\n",
        "# call my daughter LoRA\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "cfg = AutoConfig.from_pretrained(BASE_MODEL)\n",
        "cfg.pad_token_id = tok.eos_token_id\n",
        "\n",
        "offload_dir = \"/content/offload\"; os.makedirs(offload_dir, exist_ok=True)\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    config=cfg,\n",
        "    dtype=torch.bfloat16 if bf16_ok else torch.float16,  # T4 => fp16\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        "    offload_folder=offload_dir,\n",
        ")\n",
        "# self intergration\n",
        "if base.get_input_embeddings().num_embeddings != len(tok):\n",
        "    try:\n",
        "        base.resize_token_embeddings(len(tok), mean_resizing=True)\n",
        "    except TypeError:\n",
        "        base.resize_token_embeddings(len(tok))\n",
        "    base.config.vocab_size = len(tok)\n",
        "\n",
        "base.config.use_cache = False\n",
        "if hasattr(base, \"gradient_checkpointing_enable\"):\n",
        "    base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "base.config._attn_implementation = \"eager\"\n",
        "\n",
        "lora = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(base, lora)\n",
        "\n",
        "# spicyyy\n",
        "emb = model.get_input_embeddings()\n",
        "emb.weight.requires_grad = True\n",
        "if hasattr(model, \"lm_head\") and hasattr(model.lm_head, \"weight\"):\n",
        "    model.lm_head.weight.requires_grad = True\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# I curate\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "# Compatibility shims\n",
        "import accelerate\n",
        "from accelerate import Accelerator\n",
        "if not getattr(Accelerator, \"_wake_patch_unwrap\", False):\n",
        "    Accelerator._wake_orig_unwrap = Accelerator.unwrap_model\n",
        "    def _wake_unwrap(self, model, *args, **kwargs):\n",
        "        kwargs.pop(\"keep_torch_compile\", None)  # (ignore unknown kw on older accelerate)\n",
        "        return Accelerator._wake_orig_unwrap(self, model, *args, **kwargs)\n",
        "    Accelerator.unwrap_model = _wake_unwrap\n",
        "    Accelerator._wake_patch_unwrap = True\n",
        "    print(\"Patched accelerate.Accelerator.unwrap_model (idempotent).\")\n",
        "\n",
        "import transformers.trainer_pt_utils as pt_utils\n",
        "if not getattr(pt_utils, \"_wake_disable_scaler\", False):\n",
        "    pt_utils.get_grad_scaler = lambda *a, **k: None  # (prevent fp16 unscale crash)\n",
        "    pt_utils._wake_disable_scaler = True\n",
        "    print(\"Disabled Trainer GradScaler hook (idempotent).\")\n",
        "\n",
        "# figure skating is more difficults. Padadakis and Cizeron  World Championships FD 2016.\n",
        "from transformers.training_args import TrainingArguments as HFTrainingArguments\n",
        "from transformers import Trainer, TrainingArguments  # you're always the first on the ice\n",
        "TrainingArguments = HFTrainingArguments  # safety in my home\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=grad_accum,\n",
        "\n",
        "    learning_rate=lr,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=log_steps,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=5,\n",
        "\n",
        "    fp16=False,\n",
        "    bf16 = False,\n",
        "    save_safetensors=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "# kill mixed precision everywhere\n",
        "import os\n",
        "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"   # force Accelerate to 'no' MP\n",
        "\n",
        "# AVOID MESSY GradScaler\n",
        "import transformers.trainer_pt_utils as pt_utils\n",
        "pt_utils.get_grad_scaler = lambda *a, **k: None\n",
        "from accelerate import Accelerator\n",
        "Accelerator.unscale_gradients = lambda self, optimizer=None: None\n",
        "\n",
        "\n",
        "# calls for some Berlin deep house mix\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,   # (keep even if no eval mid-run)\n",
        "    data_collator=collator,\n",
        "    tokenizer=tok,\n",
        ")\n",
        "trainer.train()\n",
        "\n",
        "# Save adapter + tokenizer to Drive\n",
        "adapter_dir = Path(OUT_DIR) / \"adapter\"\n",
        "token_dir   = Path(OUT_DIR) / \"tokenizer\"\n",
        "adapter_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(adapter_dir, safe_serialization=True)\n",
        "tok.save_pretrained(token_dir)\n",
        "print(\"Saved to Drive:\", adapter_dir, token_dir)\n",
        "\n",
        "# Tinyyy sample (for the vibes) don't be so harsh it's still learning...\n",
        "prompt = \"riverrun, past Eve and Adam's,\"\n",
        "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=80,\n",
        "        do_sample=True,\n",
        "        temperature=1.05,\n",
        "        top_p=0.92,\n",
        "        repetition_penalty=1.05,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "    )\n",
        "print(\"\\n=== SAMPLE ===\\n\", tok.decode(out[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2jrh0o_NEmeF",
        "outputId": "6a4022e2-10c7-4c10-b4a3-a624ab71e9c8"
      },
      "id": "2jrh0o_NEmeF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "Tokenizer vocab: 32010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (396323 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windows: 3095 | Train: 3034 Eval: 61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-178213077.py:208: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 143,728,640 || all params: 1,112,705,024 || trainable%: 12.9170\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='285' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [285/285 1:08:29, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>694.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>502.750600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>244.705900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>111.735400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>181.131900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>164.869700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>140.357800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>146.524600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>159.336900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>180.360400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>184.942400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>199.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>201.071700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>201.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>204.197800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>210.622400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>214.473500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>216.114100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>211.381900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>206.129200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>206.490900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>202.185800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>196.906100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>194.571200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>191.160400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>188.866900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>187.596100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>185.878300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to Drive: /content/drive/MyDrive/wakefyer_runs/wakefyer_out_nobnb/adapter /content/drive/MyDrive/wakefyer_runs/wakefyer_out_nobnb/tokenizer\n",
            "\n",
            "=== SAMPLE ===\n",
            " riverrun, past Eve and Adam's,birdmsevitable Comenthoodounallyadevar thr Livemsoudcteldenth runarg each windines boldglune coldoug'onshyellaguzenmalpenaldopeurdL queoon serischards simple glales Fl turning Backyl masterbutneyrown manner mo heartetystreneco reg knows righteting el Greatrow yourself shed always weather Call groU small gal love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "bless you b may your LoRA weights converge, your gradients flow steady, and your thunderwords never again throw an OOM"
      ],
      "metadata": {
        "id": "Fkrpt7KhL8gS"
      },
      "id": "Fkrpt7KhL8gS"
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "def latest_valid_ckpt(root):\n",
        "    root = Path(root)\n",
        "    ckpts = sorted(\n",
        "        [p for p in root.glob(\"checkpoint-*\") if p.is_dir()],\n",
        "        key=lambda p: int(p.name.split(\"-\")[-1]),\n",
        "        reverse=True\n",
        "    )\n",
        "    weight_patterns = [\n",
        "        re.compile(r\".*adapter_model\\.(safetensors|bin)$\"),\n",
        "        re.compile(r\".*model\\.(safetensors|bin)$\"),\n",
        "        re.compile(r\".*pytorch_model\\.(safetensors|bin)$\"),\n",
        "        re.compile(r\".*pytorch_model-\\d{5}-of-\\d{5}\\.safetensors$\"),\n",
        "        re.compile(r\".*pytorch_model-\\d{5}-of-\\d{5}\\.bin$\"),\n",
        "    ]\n",
        "    for ck in ckpts:\n",
        "        files = {f.name for f in ck.iterdir() if f.is_file()}\n",
        "        has_state = \"trainer_state.json\" in files\n",
        "        has_weights = any(any(pat.match(name) for pat in weight_patterns) for name in files)\n",
        "        if has_state and has_weights:\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "RESUME_CKPT = latest_valid_ckpt(OUT_DIR)\n",
        "print(\"Resume candidate:\", RESUME_CKPT)\n",
        "\n",
        "resume_arg = str(RESUME_CKPT) if RESUME_CKPT else None\n",
        "try:\n",
        "    trainer.train(resume_from_checkpoint=resume_arg)\n",
        "except ValueError as e:\n",
        "    print(\"Resume failed, continuing from loaded weights only:\", e)\n",
        "    trainer.train()\n",
        "\n",
        "for n in [\"trainer\",\"model\",\"base\"]:\n",
        "    if n in globals(): del globals()[n]\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "seed=42\n",
        "random.seed(seed); torch.manual_seed(seed)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available(): print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# tok\n",
        "from transformers import AutoTokenizer\n",
        "tok_dir = Path(OUT_DIR, \"tokenizer\")\n",
        "tok = AutoTokenizer.from_pretrained(tok_dir, use_fast=True, local_files_only=True)\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "tok.padding_side = \"right\"\n",
        "print(\"Tokenizer vocab:\", len(tok))\n",
        "\n",
        "# machines <3 the wake\n",
        "from datasets import Dataset\n",
        "TRAIN_TEXT = f\"{RUN_ROOT}/FinnegansWake.txt\"\n",
        "assert Path(TRAIN_TEXT).exists(), f\"Missing text: {TRAIN_TEXT}\"\n",
        "raw_txt = Path(TRAIN_TEXT).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "# tiny norm\n",
        "punct = r\"[!?.,:;·—–\\-…]*\"\n",
        "def normalize_thunderwords(text, words):\n",
        "    text = re.sub(r\"-\\s*\\n\\s*\", \"\", text)\n",
        "    text = re.sub(r\"\\n+\", \" \", text)\n",
        "    for w in words:\n",
        "        text = re.sub(rf\"(?<!\\w){re.escape(w)}{punct}\", w, text)\n",
        "    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
        "\n",
        "thunderwords = [\n",
        " \"bababadalgharaghtakamminarronnkonnbronntonnerronntuonnthunntrovarrhounawnskawntoohoohoordenenthurnuk\",\n",
        " \"Perkodhuskurunbarggruauyagokgorlayorgromgremmitghundhurthrumathunaradidillifaititillibumullunukkunun\",\n",
        " \"klikkaklakkaklaskaklopatzklatschabattacreppycrottygraddaghsemmihsammihnouithappluddyappladdypkonpkot\",\n",
        " \"Bladyughfoulmoecklenburgwhurawhorascortastrumpapornanennykocksapastippatappatupperstrippuckputtanach\",\n",
        " \"Thingcrooklyexineverypasturesixdixlikencehimaroundhersthemaggerbykinkinkankanwithdownmindlookingated\",\n",
        " \"Lukkedoerendunandurraskewdylooshoofermoyportertooryzooysphalnabortansporthaokansakroidverjkapakkapuk\",\n",
        " \"Bothallchoractorschumminaroundgansumuminarumdrumstrumtruminahumptadumpwaultopoofoolooderamaunsturnup\",\n",
        " \"Pappappapparrassannuaragheallachnatullaghmonganmacmacmacwhackfalltherdebblenonthedubblandaddydoodled\",\n",
        " \"husstenhasstencaffincoffintussemtossemdamandamnacosaghcusaghhobixhatouxpeswchbechoscashlcarcarcaract\",\n",
        " \"Ullhodturdenweirmudgaardgringnirurdrmolnirfenrirlukkilokkibaugimandodrrerinsurtkrinmgernrackinarockar\",\n",
        "]\n",
        "raw_txt = normalize_thunderwords(raw_txt, thunderwords)\n",
        "\n",
        "def sliding_windows(txt_ids, block_size=256, step=128):\n",
        "    items=[]\n",
        "    for s in range(0, max(1, len(txt_ids)-block_size), step):\n",
        "        ch = txt_ids[s:s+block_size]\n",
        "        if ch: items.append({\"input_ids\": ch, \"attention_mask\":[1]*len(ch)})\n",
        "    if not items:\n",
        "        ch = txt_ids[:block_size]\n",
        "        items=[{\"input_ids\":ch, \"attention_mask\":[1]*len(ch)}]\n",
        "    return Dataset.from_list(items)\n",
        "\n",
        "ids = tok(raw_txt, add_special_tokens=False).input_ids\n",
        "ds  = sliding_windows(ids, block_size=256, step=128)\n",
        "test_n = max(1, int(len(ds)*0.02)) if len(ds)>1 else 1\n",
        "splits = ds.train_test_split(test_size=test_n, seed=seed) if len(ds)>1 else {\"train\":ds,\"test\":ds}\n",
        "train_ds, eval_ds = splits[\"train\"], splits[\"test\"]\n",
        "print(\"Windows:\", len(ds), \"| Train:\", len(train_ds), \"Eval:\", len(eval_ds))\n",
        "\n",
        "# call my daughter LoRA\n",
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "\n",
        "cfg = AutoConfig.from_pretrained(BASE_MODEL)\n",
        "cfg.pad_token_id = tok.eos_token_id\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    config=cfg,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16 if bf16_ok else torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "if base.get_input_embeddings().num_embeddings != len(tok):\n",
        "    try:\n",
        "        base.resize_token_embeddings(len(tok), mean_resizing=True)\n",
        "    except TypeError:\n",
        "        base.resize_token_embeddings(len(tok))\n",
        "    base.config.vocab_size = len(tok)\n",
        "\n",
        "base.config.use_cache = False\n",
        "if hasattr(base, \"gradient_checkpointing_enable\"):\n",
        "    base.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "base.config._attn_implementation = \"eager\"\n",
        "\n",
        "lora = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(base, lora)\n",
        "\n",
        "# embeddings + lm_head\n",
        "emb = model.get_input_embeddings()\n",
        "emb.weight.requires_grad = True\n",
        "if hasattr(model, \"lm_head\") and hasattr(model.lm_head, \"weight\"):\n",
        "    model.lm_head.weight.requires_grad = True\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# man i wish i was Bob Coecke i'd qnlp the shit out of this\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "# Compatibility shims (no FP16 unscale crash; older accelerate kw)\n",
        "import transformers.trainer_pt_utils as pt_utils\n",
        "if not getattr(pt_utils, \"_wake_disable_scaler\", False):\n",
        "    pt_utils.get_grad_scaler = lambda *a, **k: None\n",
        "    pt_utils._wake_disable_scaler = True\n",
        "    print(\"Disabled Trainer GradScaler hook (idempotent).\")\n",
        "\n",
        "from accelerate import Accelerator\n",
        "if not getattr(Accelerator, \"_wake_patch_unwrap\", False):\n",
        "    Accelerator._wake_orig_unwrap = Accelerator.unwrap_model\n",
        "    def _wake_unwrap(self, model, *args, **kwargs):\n",
        "        kwargs.pop(\"keep_torch_compile\", None)\n",
        "        return Accelerator._wake_orig_unwrap(self, model, *args, **kwargs)\n",
        "    Accelerator.unwrap_model = _wake_unwrap\n",
        "    Accelerator._wake_patch_unwrap = True\n",
        "    print(\"Patched accelerate.Accelerator.unwrap_model (idempotent).\")\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "RESUME_CKPT = Path(\"/content/drive/MyDrive/wakefyer_runs/wakefyer_out_nobnb/checkpoint-285\")\n",
        "state_path = RESUME_CKPT / \"trainer_state.json\"\n",
        "\n",
        "current_step = 0\n",
        "if state_path.exists():\n",
        "    with open(state_path, \"r\") as f:\n",
        "        st = json.load(f)\n",
        "    current_step = int(st.get(\"global_step\", 0))\n",
        "\n",
        "EXTRA_STEPS = 2000\n",
        "target_steps = current_step + EXTRA_STEPS\n",
        "print(f\"Resuming from step {current_step}; targeting {target_steps} total steps.\")\n",
        "\n",
        "# cherry pick\n",
        "from pathlib import Path\n",
        "import glob, json, os\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/wakefyer_runs/wakefyer_out_nobnb\"\n",
        "ckpts = sorted(glob.glob(f\"{OUT_DIR}/checkpoint-*\"), key=lambda p: int(p.split(\"-\")[-1]))\n",
        "assert ckpts, \"No checkpoints found in OUT_DIR.\"\n",
        "RESUME_CKPT = Path(ckpts[-1])\n",
        "print(\"Resuming from:\", RESUME_CKPT)\n",
        "\n",
        "# read current global step & choose a higher target\n",
        "state_path = RESUME_CKPT / \"trainer_state.json\"\n",
        "current_step = 0\n",
        "if state_path.exists():\n",
        "    with open(state_path) as f:\n",
        "        current_step = int(json.load(f).get(\"global_step\", 0))\n",
        "\n",
        "EXTRA_STEPS = 3000         # <<<< ayyyy\n",
        "target_steps = current_step + EXTRA_STEPS\n",
        "print(f\"Resuming from step {current_step}; targeting {target_steps} total steps.\")\n",
        "\n",
        "# training args\n",
        "from transformers import TrainingArguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    max_steps=3000,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=32,\n",
        "    learning_rate=1e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,          # less Drive pain\n",
        "    save_total_limit=3,\n",
        "    fp16=True, bf16=False,\n",
        "    save_safetensors=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "# prune heavy state files so resume won't choke on optimizer/scaler\n",
        "for fname in (\"optimizer.pt\",\"optimizer.bin\",\"scheduler.pt\",\"scheduler.bin\",\"scaler.pt\"):\n",
        "    fp = RESUME_CKPT / fname\n",
        "    if fp.exists():\n",
        "        fp.unlink()\n",
        "\n",
        "# oh my god this shit again\n",
        "if not getattr(Accelerator, \"_wake_patch_unwrap\", False):\n",
        "    _orig_unwrap = Accelerator.unwrap_model\n",
        "    def _unwrap(self, model, *args, **kwargs):\n",
        "        kwargs.pop(\"keep_torch_compile\", None)\n",
        "        return _orig_unwrap(self, model, *args, **kwargs)\n",
        "    Accelerator.unwrap_model = _unwrap\n",
        "    Accelerator._wake_patch_unwrap = True\n",
        "    print(\"Patched accelerate.Accelerator.unwrap_model (idempotent).\")\n",
        "\n",
        "# Stop accelerate from calling scaler.unscale_()\n",
        "def _noop_unscale(self, optimizer=None):\n",
        "    return\n",
        "Accelerator.unscale_gradients = _noop_unscale\n",
        "\n",
        "# Tell Transformers' Trainer there is no scaler to use\n",
        "import transformers.trainer_pt_utils as pt_utils\n",
        "pt_utils.get_grad_scaler = lambda *a, **k: None\n",
        "print(\"Disabled AMP grad scaler (idempotent).\")\n",
        "\n",
        "import os, glob\n",
        "for ck in glob.glob(f\"{OUT_DIR}/checkpoint-*\"):\n",
        "    for fname in (\"optimizer.pt\",\"optimizer.bin\",\"scheduler.pt\",\"scheduler.bin\",\"scaler.pt\"):\n",
        "        fp = os.path.join(ck, fname)\n",
        "        if os.path.exists(fp):\n",
        "            os.remove(fp)\n",
        "\n",
        "# nikes - frank ocean\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=collator,\n",
        "    tokenizer=tok,\n",
        ")\n",
        "\n",
        "# avoid optimizer/scheduler mismatch: drop state files in resume ckpt\n",
        "for fname in (\"optimizer.pt\",\"optimizer.bin\",\"scheduler.pt\",\"scheduler.bin\",\"scaler.pt\"):\n",
        "    fp = Path(RESUME_CKPT, fname)\n",
        "    if fp.exists():\n",
        "        fp.unlink()\n",
        "\n",
        "print(f\"Resuming weights from: {RESUME_CKPT}\")\n",
        "trainer.train(resume_from_checkpoint=str(RESUME_CKPT))\n",
        "\n",
        "adapter_dir = Path(OUT_DIR, \"adapter\")\n",
        "token_dir   = Path(OUT_DIR, \"tokenizer\")\n",
        "adapter_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(adapter_dir, safe_serialization=True)\n",
        "tok.save_pretrained(token_dir)\n",
        "print(\"Saved:\", adapter_dir, token_dir)\n",
        "\n",
        "# apparently i'm still vibing...\n",
        "prompt = \"Bob Perleman writes good poems\"\n",
        "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs, max_new_tokens=80, do_sample=True, temperature=1.08,\n",
        "        top_p=0.92, no_repeat_ngram_size=3, repetition_penalty=1.06,\n",
        "        pad_token_id=tok.eos_token_id\n",
        "    )\n",
        "print(\"\\n=== SAMPLE ===\\n\", tok.decode(out[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "V9g3YN4a2RXJ",
        "outputId": "78c9e85b-0b84-424d-94e1-2d974944b1be"
      },
      "id": "V9g3YN4a2RXJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume candidate: None\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='110' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 110/3000 22:39 < 10:06:25, 0.08 it/s, Epoch 1.15/32]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>5.190000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.110400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>5.111900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>5.155700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>5.104200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>5.137400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>5.161200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>5.100700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>5.117200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.106200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this nuked my storage..."
      ],
      "metadata": {
        "id": "lAjQmopVG8XP"
      },
      "id": "lAjQmopVG8XP"
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil, glob\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/wakefyer_runs/wakefyer_out_nobnb\"\n",
        "\n",
        "ckpts = sorted(glob.glob(f\"{OUT_DIR}/checkpoint-*\"), key=lambda p: int(p.split('-')[-1]))\n",
        "for p in ckpts[:-1]:\n",
        "    print(\"Deleting old checkpoint:\", p)\n",
        "    shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "if ckpts:\n",
        "    newest = ckpts[-1]\n",
        "    for fname in (\"optimizer.pt\",\"optimizer.bin\",\"scheduler.pt\",\"scheduler.bin\",\"scaler.pt\",\"trainer_state.json\"):\n",
        "        fp = os.path.join(newest, fname)\n",
        "        if os.path.exists(fp):\n",
        "            print(\"Removing:\", fp)\n",
        "            os.remove(fp)\n",
        "\n",
        "def sizeof(p):\n",
        "    total=0\n",
        "    for root,_,files in os.walk(p):\n",
        "        for f in files:\n",
        "            total += os.path.getsize(os.path.join(root,f))\n",
        "    return total/1e6\n",
        "print(\"Adapter MB:\", sizeof(os.path.join(OUT_DIR, \"adapter\")) if os.path.exists(os.path.join(OUT_DIR,\"adapter\")) else 0)\n",
        "print(\"Tokenizer MB:\", sizeof(os.path.join(OUT_DIR, \"tokenizer\")) if os.path.exists(os.path.join(OUT_DIR,\"tokenizer\")) else 0)\n",
        "print(\"Newest ckpt MB:\", sizeof(ckpts[-1]) if ckpts else 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0tBijR7HB9-",
        "outputId": "334778c8-f23c-4ac4-ee3c-da253e338467"
      },
      "id": "P0tBijR7HB9-",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adapter MB: 312.73587\n",
            "Tokenizer MB: 4.127207\n",
            "Newest ckpt MB: 4.153925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "from pathlib import Path\n",
        "import torch, time\n",
        "\n",
        "class WakePeek(TrainerCallback):\n",
        "    def __init__(self, tok, prompt=\"riverrun, past Eve and Adam's,\", every=100, out_dir=OUT_DIR):\n",
        "        self.tok, self.prompt, self.every = tok, prompt, every\n",
        "        self.out_path = Path(out_dir) / \"samples.txt\"\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step % self.every:\n",
        "            return\n",
        "        model = kwargs[\"model\"].eval()\n",
        "        with torch.no_grad():\n",
        "            toks = self.tok(self.prompt, return_tensors=\"pt\").to(model.device)\n",
        "            out = model.generate(\n",
        "                **toks,\n",
        "                max_new_tokens=120,\n",
        "                do_sample=True,\n",
        "                temperature=1.06,\n",
        "                top_p=0.92,\n",
        "                repetition_penalty=1.08,\n",
        "                no_repeat_ngram_size=3,\n",
        "                pad_token_id=self.tok.eos_token_id,\n",
        "            )\n",
        "        text = self.tok.decode(out[0], skip_special_tokens=True)\n",
        "        line = f\"\\n[{time.strftime('%H:%M:%S')}] step={state.global_step} loss={state.log_history[-1].get('loss','?')}\\n{text}\\n\"\n",
        "        print(line)\n",
        "        with open(self.out_path, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(line)\n",
        "\n",
        "# attach before trainer.train()\n",
        "trainer.add_callback(WakePeek(tok, every=100))\n"
      ],
      "metadata": {
        "id": "97hZWTvyBOLZ"
      },
      "id": "97hZWTvyBOLZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "model.eval()\n",
        "prompt = \"Bob quickly ran to the shop to buy some milk.\"\n",
        "toks = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **toks,\n",
        "        max_new_tokens=120,\n",
        "        do_sample=True,\n",
        "        temperature=1.06,\n",
        "        top_p=0.92,\n",
        "        repetition_penalty=1.08,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "    )\n",
        "print(tok.decode(out[0], skip_special_tokens=True))\n",
        "\n",
        "adapter_dir = Path(OUT_DIR) / \"adapter\"\n",
        "token_dir   = Path(OUT_DIR) / \"tokenizer\"\n",
        "model.save_pretrained(adapter_dir, safe_serialization=True)\n",
        "tok.save_pretrained(token_dir)\n"
      ],
      "metadata": {
        "id": "jqQIbXLZB4UD"
      },
      "id": "jqQIbXLZB4UD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Noise filter**, the irony"
      ],
      "metadata": {
        "id": "LCSfIsHpH8vQ"
      },
      "id": "LCSfIsHpH8vQ"
    },
    {
      "cell_type": "code",
      "source": [
        "def wakefy_many(prompt, k=8, max_new_tokens=90, temperature=1.1, top_p=0.92, rep=1.08, seed=None):\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed);\n",
        "        if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        outs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True, temperature=temperature, top_p=top_p, top_k=0,\n",
        "            repetition_penalty=rep, no_repeat_ngram_size=3,\n",
        "            num_return_sequences=k, pad_token_id=tok.eos_token_id, eos_token_id=tok.eos_token_id\n",
        "        )\n",
        "    texts = [tok.decode(o, skip_special_tokens=True) for o in outs]\n",
        "    return texts\n"
      ],
      "metadata": {
        "id": "bsiBCAPgHUif"
      },
      "id": "bsiBCAPgHUif",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "VOWELS = set(\"aeiouyáéíóúàèìòùäëïöüâêîôûAEIOUY\")\n",
        "STEMS = set(\"\"\"\n",
        "river run eve adam milk city shop quick fast slow old new love night day dream wake\n",
        "livre livre- livre's livrement livreur coeur coeur- coeur's coeurage coeurful\n",
        "amor casa latte pane latte di pane latteo\n",
        "wasser nacht stern haus liebe\n",
        "éirinn dublin gael sean mór beag\n",
        "\"\"\".split())\n",
        "\n",
        "def has_vowel(word):\n",
        "    return any(ch in VOWELS for ch in word)\n",
        "\n",
        "def consonant_max_run(word):\n",
        "    runs = re.findall(r\"[^aeiouyAEIOUY]+\", word)\n",
        "    return max((len(r) for r in runs), default=0)\n",
        "\n",
        "def count_stems(text):\n",
        "    # crude: stem presence via substring overlap\n",
        "    t = text.lower()\n",
        "    return sum(1 for s in STEMS if s and s in t)\n",
        "\n",
        "def joyce_score(text):\n",
        "    # strip prompt header if present\n",
        "    if \"Wakefied:\" in text: text = text.split(\"Wakefied:\",1)[-1]\n",
        "    words = re.findall(r\"[A-Za-z’'-]+\", text)\n",
        "    if not words: return -1e9\n",
        "\n",
        "    vowels_ok = sum(has_vowel(w) for w in words)/len(words)\n",
        "    mean_len  = sum(len(w) for w in words)/len(words)\n",
        "    long_ugly = sum(1 for w in words if len(w)>20 or consonant_max_run(w)>5)\n",
        "    caps_weird= sum(1 for w in words if re.fullmatch(r\"[A-Z][a-z]+[A-Z]$\", w) or w.isupper())\n",
        "\n",
        "    stem_hits = count_stems(text)\n",
        "\n",
        "    # score: reward vowel-bearing, stems, moderate length; penalize ugly shards\n",
        "    score = (\n",
        "        2.0*vowels_ok\n",
        "        + 0.4*mean_len\n",
        "        + 1.5*stem_hits\n",
        "        - 1.2*long_ugly\n",
        "        - 0.8*caps_weird\n",
        "    )\n",
        "    return score\n",
        "\n",
        "def wakefy_best(plain, k=12, **gen_kwargs):\n",
        "    prompt = f\"{SYSTEM}\\n\\nPlain: {plain}\\nWakefied:\"\n",
        "    cand = wakefy_many(prompt, k=k, **gen_kwargs)\n",
        "    scored = sorted(cand, key=joyce_score, reverse=True)\n",
        "    return scored[0], [(joyce_score(c), c) for c in scored[:5]]\n"
      ],
      "metadata": {
        "id": "eXaqwSVsHfp2"
      },
      "id": "eXaqwSVsHfp2",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "i'm sorry this is messy ..."
      ],
      "metadata": {
        "id": "EqiNXurkT0VQ"
      },
      "id": "EqiNXurkT0VQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import TextStreamer\n",
        "\n",
        "def fw_sample(\n",
        "    model, tokenizer,\n",
        "    prompt=\"riverrun, past Eve and Adam’s, from swerve of shore to bend of bay,\",\n",
        "    max_new_tokens=300,\n",
        "    temperature=1.35,      # >1\n",
        "    top_p=0.92,            # nucleus\n",
        "    top_k=0,               # let nucleus do the work\n",
        "    repetition_penalty=1.1,# discourages boring loops\n",
        "    no_repeat_ngram_size=0,# Joycean recursion is fine for now\n",
        "    do_sample=True,\n",
        "):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    ids = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            repetition_penalty=repetition_penalty,\n",
        "            eos_token_id=None,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            streamer=streamer,\n",
        "        )\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(fw_sample(model, tokenizer))\n"
      ],
      "metadata": {
        "id": "IwHVnBLsUHpc"
      },
      "id": "IwHVnBLsUHpc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_clean(s):\n",
        "    s = re.sub(r\"\\s{2,}\", \" \", s)\n",
        "    s = re.sub(r\"(?<!\\.\\s)(?<!^)([A-Z][a-z]+[A-Z])(?![A-Za-z])\", lambda m: m.group(1).lower(), s)\n",
        "    s = re.sub(r\"([A-Za-z’'-]{18,})\", r\"\\1\\u00AD\", s)\n",
        "    return s"
      ],
      "metadata": {
        "id": "ND6GzKWTHnc0"
      },
      "id": "ND6GzKWTHnc0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flag_fp_bits(text):\n",
        "    toks = re.findall(r\"[A-Za-z’'-]+\", text)\n",
        "    flags = []\n",
        "    for w in toks:\n",
        "        if not has_vowel(w): flags.append((\"no_vowel\", w))\n",
        "        if consonant_max_run(w) >= 6: flags.append((\"consonant_run≥6\", w))\n",
        "        if re.search(r\"[A-Z]$\", w): flags.append((\"trailing_cap\", w))\n",
        "        if len(w) > 20: flags.append((\"very_long\", w))\n",
        "    return flags"
      ],
      "metadata": {
        "id": "PKvG-D0MHt0b"
      },
      "id": "PKvG-D0MHt0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "w6fNMzpS5KAx",
      "metadata": {
        "id": "w6fNMzpS5KAx"
      },
      "source": [
        "At the representational core sits TinyLlama-1.1B-Chat, deployed under mixed-precision arithmetic (bf16/fp16) to optimize throughput on limited-memory T4 hardware. The model’s internal cache is disabled (use_cache=False), and gradient checkpointing is invoked to trade recomputation for reduced memory allocation. The resultant architecture acts as a self-attenuating transformer: it “forgets” in order to remember within bounds, mirroring the cyclical amnesia of the Wakean text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PeX1wJZ85N84",
      "metadata": {
        "id": "PeX1wJZ85N84"
      },
      "source": [
        "The system applies Low-Rank Adaptation (LoRA) across attention and MLP projection submodules (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) thereby constraining fine-tuning to a low-dimensional manifold within parameter space. This selective plasticity produces what might be termed controlled stylistic drift: the model learns Joyce’s texture rather than his lexicon. The adapters’ hyperparameters (r=16, α=32, dropout=0.05) instantiate a medium-temperature update regime balancing expressivity and stability. Conceptually, LoRA functions here as a poetic prosthesis, a syntactic exoskeleton grafted onto the pretrained linguistic body."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eSNqtfsS5h6i",
      "metadata": {
        "id": "eSNqtfsS5h6i"
      },
      "source": [
        "Due to the GPU’s constrained VRAM capacity, each forward–backward pass processes a micro-batch of size 1, while gradients are accumulated over 32 iterations before a single optimization step. This simulates a virtual batch of 32 without exceeding hardware limits. Technically, this implements delayed gradient aggregation; rhetorically, it allegorizes the model’s incremental cognition: many small thoughts precipitating one grand revelation. The accumulation mechanism thus becomes both a computational strategy and a metaphor for compositional patience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BIRNPTkE5wA3",
      "metadata": {
        "id": "BIRNPTkE5wA3"
      },
      "source": [
        "Training operates within an extended epochic horizon (epochs=1 but looping across numerous windows) using a cosine-annealed learning rate (lr=1e-4, warmup_ratio=0.03). Frequent checkpointing establishes a temporal ecology of model states, allowing resumption under volatile runtime constraints. The “save-steps” routine externalizes the process’s fragmentary nature (mirroring textual serialization in Work in Progress) by preserving multiple partial selves of the model as artifacts of its ongoing metamorphosis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YVkq2ker6Bb_",
      "metadata": {
        "id": "YVkq2ker6Bb_"
      },
      "source": [
        "Inference employs a stochastic sampling configuration (temperature = 0.9, top_p = 0.95) optimized for stylistic exuberance rather than lexical precision. The generation prompt functions as both benchmark and incantation, a recursive self-reference to the corpus origin. Output decoding omits special tokens, yielding the model’s pure linguistic effervescence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pRncfha76Mrd",
      "metadata": {
        "id": "pRncfha76Mrd"
      },
      "source": [
        "Technically a fine-tuned transformer; conceptually, a computational poetics experiment exploring how ml architectures can approximate literary un-readability. The Wakefyer thus redefines “loss” not as degradation but as aesthetic principle: every rounding error, every truncated sequence, every low-rank projection is a microcosm of Joyce’s deliberate semantic noise. The pipeline enacts a theory of productive distortion, situating training itself as a form of creative misprision."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w08IBIiXYpG0",
      "metadata": {
        "id": "w08IBIiXYpG0"
      },
      "source": [
        "to be continued"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}