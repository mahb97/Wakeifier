{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wakeifier/blob/main/Wakefyer_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61d891b",
      "metadata": {
        "id": "b61d891b"
      },
      "source": [
        "# Wakefyer (Colab) — LoRA fine-tune with conservative text handling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad57a90f",
      "metadata": {
        "id": "ad57a90f"
      },
      "source": [
        "\n",
        "Runnit: from top to bottom except of all the cells that CLEARLY DONT WORK.\n",
        "Over-clean the Wake and you'll make Joyce turn in his grave (he told me this via the tarot cards): by default, doesn't normalise or strip diacritics (ultra-light cleaning toggle is included to fill space, dont use, why would you do that)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7240421",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "a7240421",
        "outputId": "f9e9cd93-8e75-4bfd-8b65-02194d722044",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'cached_property' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2330359157.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mDataCollatorForLanguageModeling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Integrations must be imported before ML frameworks:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# isort: off\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m from .integrations import (\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mget_reporting_integration_callbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mhp_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0m_has_neptune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodelcard\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_callback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgressCallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainerCallback\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPREFIX_CHECKPOINT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBestRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntervalStrategy\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modelcard.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mMODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m from .utils import (\n\u001b[1;32m     49\u001b[0m     \u001b[0mMODEL_CARD_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mSchedulerType\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mACCELERATE_MIN_VERSION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_property' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os, json, math, random, gc, sys, time\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# swap for Llama if you wanna\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "TRAIN_TEXT = \"/content/FinnegansWake.txt\"\n",
        "OUT_DIR    = \"/content/wakefyer_out\"                # checkpoints, adapter, tokenizer\n",
        "BLOCK_SIZE = 1024\n",
        "STRIDE     = 128                                    # token overlap\n",
        "EPOCHS     = 1\n",
        "LR         = 2e-4\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM = 16\n",
        "BF16       = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8  # A100/TPU/modern GPUs\n",
        "LOAD_8BIT  = True                                   # set False if you hit bnb issues\n",
        "LOG_STEPS  = 10\n",
        "EVAL_RATIO = 0.02                                   # mini eval split\n",
        "SEED       = 42\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42799ac2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42799ac2",
        "outputId": "b463a567-7cbf-4e89-89b2-a665567ffd95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive mounted. Outputs can be copied to: /content/drive/MyDrive/wake_runs\n"
          ]
        }
      ],
      "source": [
        "# (Optional) Mount Google Drive to save outputs / read data\n",
        "USE_DRIVE = True\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=False)\n",
        "    DRIVE_OUT = \"/content/drive/MyDrive/wake_runs\"\n",
        "    os.makedirs(DRIVE_OUT, exist_ok=True)\n",
        "    print(\"Drive mounted. Outputs can be copied to:\", DRIVE_OUT)\n",
        "else:\n",
        "    DRIVE_OUT = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f31b56a",
      "metadata": {
        "id": "0f31b56a"
      },
      "source": [
        "### Load text with **no destructive cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dfbde98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "8dfbde98",
        "outputId": "1d1e3002-9032-4b39-9174-c183febcc3fb",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Path' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2768854825.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_TEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Upload your text to {TRAIN_TEXT} (Files pane > Upload)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_TEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"none\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# change to 'ultra_light' if you *reallyyyy* need it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loaded chars:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Path' is not defined"
          ]
        }
      ],
      "source": [
        "from typing import Literal\n",
        "\n",
        "def read_text(path: str, cleaning: Literal[\"none\",\"ultra_light\"]=\"none\") -> str:\n",
        "    # Read raw — keep dem features\n",
        "    txt = Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    if cleaning == \"ultra_light\":\n",
        "        # normalise newlines and strip BOM; don't fold case, remove accents, replace punctuation, etc. Can't believe in need to say this lol\n",
        "        txt = txt.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
        "        if txt.startswith(\"\\ufeff\"):  # BOM\n",
        "            txt = txt.lstrip(\"\\ufeff\")\n",
        "        # how about not collapsing repeated spaces or altering hyphenation/spellings\n",
        "    return txt\n",
        "\n",
        "assert Path(TRAIN_TEXT).exists(), f\"Upload your text to {TRAIN_TEXT} (Files pane > Upload)\"\n",
        "text = read_text(TRAIN_TEXT, cleaning=\"none\")  # change to 'ultra_light' if you *reallyyyy* need it\n",
        "print(\"Loaded chars:\", len(text))\n",
        "print(\"Sample head (5000 chars):\\n\", text[:5000])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"datasets==2.20.0\" \"sentencepiece==0.2.0\"\n",
        "import os, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True,max_split_size_mb:64\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# this will moan about compatibility...\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyhmdJkckSFn",
        "outputId": "03e022fc-9c0a-4a8a-bdf1-068af43cc43e"
      },
      "id": "VyhmdJkckSFn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A working cell for broke girls with no mula for colab and who dont wanna chug money into the firey pit that is hell that is google that is oppression.**\n",
        "\n",
        "The Wakefyer fine-tuning architecture constitutes a hybridized LoRA-based low-rank adaptation pipeline for stylistic transformation within a bfloat16/fp16 mixed-precision environment. Its design paradigm is deliberately antithetical to large-scale quantized optimization frameworks such as bitsandbytes, privileging instead interpretability, traceability, and computational minimalism. Conceptually, it operates as a micro-epistemic engine: a system whose purpose is not semantic fidelity but stylistic distortion, functioning as a textual “entropy amplifier” with respect to Finnegans Wake’s logorrheic poetics."
      ],
      "metadata": {
        "id": "raeXSe4f2qen"
      },
      "id": "raeXSe4f2qen"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text ingestion follows a non-normalizing, non-canonical pipeline, thereby maintaining orthographic heterogeneity as an essential stylistic feature. The raw Joycean corpus is tokenized through a fast SentencePiece model and segmented using overlapping sliding windows defined by max_len and stride. This overlap emulates a continuum of narrativity (each window partially repeating the previous one) producing a computational analogue of Joyce’s recursive syntactic rhythm. No linguistic cleaning or lemmatization occurs; lexical noise is treated as signal. The preprocessing stage therefore performs not sanitization but syntactic fractalization, generating a dataset of semi-redundant micro-contexts to condition the model’s representational drift."
      ],
      "metadata": {
        "id": "ZbzxYifJ4Kmn"
      },
      "id": "ZbzxYifJ4Kmn"
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "because code should also stream-of-consciousness. avoid the binary bore.\n",
        "\n",
        "Colab prep (fresh runtime):\n",
        "!pip -q install \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"datasets==2.20.0\" \"sentencepiece==0.2.0\"\n",
        "\"\"\"\n",
        "import os, random, sys, gc\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# more animals - my tiny llamas assemble\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "TRAIN_TEXT = \"/content/FinnegansWake.txt\"           # gospel\n",
        "OUT_DIR    = \"/content/wakefyer_out_nobnb\"          # the shards of my heart\n",
        "max_len    = 256       # isuesssss\n",
        "stride     = 128       # overlapsss\n",
        "epochs     = 1         # one day in Dublin; add more if the pub stays open\n",
        "lr         = 1e-4      # gentle she says\n",
        "batch_size = 1         # because T4s have fragile little snowflake egos\n",
        "grad_accum = 32        # pretends the batch is bigger; shh....(keeps a T4 alive when fine-tuning a 1B-parameter model. We can cry about this collectively, meet me under the moon.)\n",
        "log_steps  = 10\n",
        "eval_ratio = 0.02\n",
        "seed       = 42\n",
        "\n",
        "# name my daughter LoRA\n",
        "lora_r = 16\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.05\n",
        "lora_targets = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "\n",
        "# Incantations against OOM\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:64\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def panic(msg):\n",
        "    print(f\"\\n[Wakefyer shrieks] {msg}\\n\", file=sys.stderr)\n",
        "\n",
        "# Sanity bells and whistles (is there a GPU? is it sulking?)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Validate the gospel\n",
        "if not Path(TRAIN_TEXT).exists():\n",
        "    panic(f\"the gospel'{TRAIN_TEXT}' is missing.\")\n",
        "    raise SystemExit(2)\n",
        "\n",
        "raw_txt = Path(TRAIN_TEXT).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "if len(raw_txt.strip()) < 1000:\n",
        "    panic(\"Joyce did not write haikus.\")\n",
        "\n",
        "# oh my god tell me more about your token magic\n",
        "tok = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL, use_fast=True\n",
        ")\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "tok.padding_side = \"right\"\n",
        "\n",
        "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "\n",
        "# summons the devil (no batsandbites; just vibessss coz im a witch)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.bfloat16 if bf16_ok else torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "# optional: sometimes SDPA gets capped on T4—try eager if you OOM\n",
        "# model.config._attn_implementation = \"eager\" (af)\n",
        "\n",
        "lora = LoraConfig(\n",
        "    r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
        "    target_modules=lora_targets, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# pace urself\n",
        "ids = tok(raw_txt).input_ids\n",
        "\n",
        "def sliding_windows(txt_ids, block_size=max_len, step=stride):\n",
        "    if block_size <= 0:\n",
        "        panic(\"max_len is non‑positive. Even Joyce needs at least a token.\")\n",
        "        raise ValueError(\"max_len must be > 0\")\n",
        "    items = []\n",
        "    # dont trip bro\n",
        "    for start in range(0, max(1, len(txt_ids) - block_size), step):\n",
        "        chunk = txt_ids[start:start+block_size]\n",
        "        if not chunk:\n",
        "            continue\n",
        "        items.append({\"input_ids\": chunk, \"attention_mask\": [1]*len(chunk)})\n",
        "    # what failure looks like when its not me\n",
        "    if not items:\n",
        "        chunk = txt_ids[:block_size]\n",
        "        if not chunk:\n",
        "            panic(\"After tokenization, we got zero tokens. Perhaps the tokenizer dont like Joyce.\")\n",
        "            raise ValueError(\"Empty token sequence\")\n",
        "        items = [{\"input_ids\": chunk, \"attention_mask\": [1]*len(chunk)}]\n",
        "    return Dataset.from_list(items)\n",
        "\n",
        "ds = sliding_windows(ids, block_size=max_len, step=stride)\n",
        "n = len(ds)\n",
        "print(\"Windows:\", n)\n",
        "if n < 10:\n",
        "    print(\"Consider lowering max_len or adding text for more windows.\")\n",
        "\n",
        "test_n = max(1, int(n * eval_ratio)) if n > 1 else 1\n",
        "splits = ds.train_test_split(test_size=test_n, seed=seed) if n > 1 else {\"train\": ds, \"test\": ds}\n",
        "train_ds, eval_ds = splits[\"train\"], splits[\"test\"]\n",
        "print(\"Train windows:\", len(train_ds), \"Eval windows:\", len(eval_ds))\n",
        "\n",
        "# what do you know about masking\n",
        "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "# be our guest, be our guest\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "    num_train_epochs=9999,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,               # i do this while listening to disney soundtracks does that make you cry.\n",
        "    save_total_limit=5,\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_safetensors=True,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=grad_accum,\n",
        "    num_train_epochs=epochs,\n",
        "    learning_rate=lr,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=log_steps,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    bf16=bf16_ok,\n",
        "    fp16=(not bf16_ok),\n",
        "    report_to=[],\n",
        "\n",
        ")\n",
        "\n",
        "# light at the end of the tunnel\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "try:\n",
        "    last_ckpt = OUT_DIR if Path(OUT_DIR, \"trainer_state.json\").exists() else None\n",
        "    trainer.train(resume_from_checkpoint=last_ckpt)\n",
        "except RuntimeError as e:\n",
        "    # likely OOM; I confess and suggest asceticism\n",
        "    panic(f\"Runtime groan: {e}\")\n",
        "    panic(\"Try: lower max_len (192 or 128), set model.config._attn_implementation='eager', increase grad_accum, or reduce LoRA r/alpha.\")\n",
        "    raise\n",
        "\n",
        "adapter_dir = Path(OUT_DIR) / \"adapter\"\n",
        "token_dir = Path(OUT_DIR) / \"tokenizer\"\n",
        "adapter_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(adapter_dir, safe_serialization=True)\n",
        "tok.save_pretrained(token_dir)\n",
        "print(\"Saved:\", adapter_dir, token_dir)\n",
        "\n",
        "# a test for meee\n",
        "prompt = \"riverrun, past Eve and Adam's,\"\n",
        "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=64, do_sample=True, temperature=0.9, top_p=0.95,\n",
        "        pad_token_id=tok.eos_token_id,\n",
        "    )\n",
        "print(\"\\n=== SAMPLE ===\\n\", tok.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "n--Gtpfoq_YX",
        "outputId": "c70900c1-de38-4d8f-b8f6-4f83f9771219"
      },
      "id": "n--Gtpfoq_YX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (430236 > 2048). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windows: 3360\n",
            "Train windows: 3293 Eval windows: 67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [102/102 14:44, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/wakefyer_out_nobnb/adapter /content/wakefyer_out_nobnb/tokenizer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SAMPLE ===\n",
            " riverrun, past Eve and Adam's,\n",
            "and down to the riverbed. How! That's not the first\n",
            "time you've been at it. Pardon me, my young one, and I won't\n",
            "give you away. It's not me, the old one, they say you are, that\n",
            "I'm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the representational core sits TinyLlama-1.1B-Chat, deployed under mixed-precision arithmetic (bf16/fp16) to optimize throughput on limited-memory T4 hardware. The model’s internal cache is disabled (use_cache=False), and gradient checkpointing is invoked to trade recomputation for reduced memory allocation. The resultant architecture acts as a self-attenuating transformer: it “forgets” in order to remember within bounds, mirroring the cyclical amnesia of the Wakean text."
      ],
      "metadata": {
        "id": "w6fNMzpS5KAx"
      },
      "id": "w6fNMzpS5KAx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The system applies Low-Rank Adaptation (LoRA) across attention and MLP projection submodules (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) thereby constraining fine-tuning to a low-dimensional manifold within parameter space. This selective plasticity produces what might be termed controlled stylistic drift: the model learns Joyce’s texture rather than his lexicon. The adapters’ hyperparameters (r=16, α=32, dropout=0.05) instantiate a medium-temperature update regime balancing expressivity and stability. Conceptually, LoRA functions here as a poetic prosthesis, a syntactic exoskeleton grafted onto the pretrained linguistic body."
      ],
      "metadata": {
        "id": "PeX1wJZ85N84"
      },
      "id": "PeX1wJZ85N84"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the GPU’s constrained VRAM capacity, each forward–backward pass processes a micro-batch of size 1, while gradients are accumulated over 32 iterations before a single optimization step. This simulates a virtual batch of 32 without exceeding hardware limits. Technically, this implements delayed gradient aggregation; rhetorically, it allegorizes the model’s incremental cognition: many small thoughts precipitating one grand revelation. The accumulation mechanism thus becomes both a computational strategy and a metaphor for compositional patience."
      ],
      "metadata": {
        "id": "eSNqtfsS5h6i"
      },
      "id": "eSNqtfsS5h6i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training operates within an extended epochic horizon (epochs=1 but looping across numerous windows) using a cosine-annealed learning rate (lr=1e-4, warmup_ratio=0.03). Frequent checkpointing establishes a temporal ecology of model states, allowing resumption under volatile runtime constraints. The “save-steps” routine externalizes the process’s fragmentary nature (mirroring textual serialization in Work in Progress) by preserving multiple partial selves of the model as artifacts of its ongoing metamorphosis."
      ],
      "metadata": {
        "id": "BIRNPTkE5wA3"
      },
      "id": "BIRNPTkE5wA3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference employs a stochastic sampling configuration (temperature = 0.9, top_p = 0.95) optimized for stylistic exuberance rather than lexical precision. The generation prompt functions as both benchmark and incantation, a recursive self-reference to the corpus origin. Output decoding omits special tokens, yielding the model’s pure linguistic effervescence."
      ],
      "metadata": {
        "id": "YVkq2ker6Bb_"
      },
      "id": "YVkq2ker6Bb_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technically a fine-tuned transformer; conceptually, a computational poetics experiment exploring how ml architectures can approximate literary un-readability. The Wakefyer thus redefines “loss” not as degradation but as aesthetic principle: every rounding error, every truncated sequence, every low-rank projection is a microcosm of Joyce’s deliberate semantic noise. The pipeline enacts a theory of productive distortion, situating training itself as a form of creative misprision."
      ],
      "metadata": {
        "id": "pRncfha76Mrd"
      },
      "id": "pRncfha76Mrd"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/wakefyer_runs/wakefyer_out_nobnb\"\n",
        "\n",
        "# checkpoint\n",
        "ckpts = [d for d in os.listdir(OUT_DIR) if d.startswith(\"checkpoint-\")]\n",
        "ckpt_path = os.path.join(OUT_DIR, sorted(ckpts, key=lambda s: int(re.findall(r\"\\d+\", s)[0]))[-1])\n",
        "\n",
        "# reloads base & LoRA adapter from the checkpoint\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(base, ckpt_path)\n",
        "\n",
        "# rebuild dis shit\n",
        "trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=eval_ds, data_collator=collator)\n",
        "\n",
        "# conts\n",
        "trainer.train(resume_from_checkpoint=ckpt_path)\n"
      ],
      "metadata": {
        "id": "nzq3NfcS-aUa"
      },
      "id": "nzq3NfcS-aUa",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}