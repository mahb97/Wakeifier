{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wakeifier/blob/main/Wakefyer_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b61d891b",
      "metadata": {
        "id": "b61d891b"
      },
      "source": [
        "# Wakefyer (Colab) — LoRA fine-tune with conservative text handling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad57a90f",
      "metadata": {
        "id": "ad57a90f"
      },
      "source": [
        "\n",
        "Runnit: from top to bottom except of all the cells that CLEARLY DONT WORK.\n",
        "Over-clean the Wake and you'll make Joyce turn in his grave (he told me this via the tarot cards): by default, doesn't normalise or strip diacritics (ultra-light cleaning toggle is included to fill space, dont use, why would you do that)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"datasets==2.20.0\" \"sentencepiece==0.2.0\"\n",
        "!pip install -U \"transformers>=4.46.0\"\n",
        "import os, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]=\"expandable_segments:True,max_split_size_mb:64\"\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "# this will moan about compatibility...\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyhmdJkckSFn",
        "outputId": "229fceea-83f8-489d-c724-287f59da64a4"
      },
      "id": "VyhmdJkckSFn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting transformers>=4.46.0\n",
            "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (2.32.4)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.46.0)\n",
            "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=4.46.0) (2025.10.5)\n",
            "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
            "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "\u001b[33m    WARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.12/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.22.1 transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers; print(transformers.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdn-pTPRvlY5",
        "outputId": "63df1eda-6f8d-477b-dab1-fac718e5b669"
      },
      "id": "Jdn-pTPRvlY5",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.57.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**A working cell for broke girls with no mula for colab and who dont wanna chug money into the firey pit that is hell that is google that is oppression.**\n",
        "\n",
        "The Wakefyer fine-tuning architecture constitutes a hybridized LoRA-based low-rank adaptation pipeline for stylistic transformation within a bfloat16/fp16 mixed-precision environment. Its design paradigm is deliberately antithetical to large-scale quantized optimization frameworks such as bitsandbytes, privileging instead interpretability, traceability, and computational minimalism. Conceptually, it operates as a micro-epistemic engine: a system whose purpose is not semantic fidelity but stylistic distortion, functioning as a textual “entropy amplifier” with respect to Finnegans Wake’s logorrheic poetics."
      ],
      "metadata": {
        "id": "raeXSe4f2qen"
      },
      "id": "raeXSe4f2qen"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text ingestion follows a non-normalizing, non-canonical pipeline, thereby maintaining orthographic heterogeneity as an essential stylistic feature. The raw Joycean corpus is tokenized through a fast SentencePiece model and segmented using overlapping sliding windows defined by max_len and stride. This overlap emulates a continuum of narrativity (each window partially repeating the previous one) producing a computational analogue of Joyce’s recursive syntactic rhythm. No linguistic cleaning or lemmatization occurs; lexical noise is treated as signal. The preprocessing stage therefore performs not sanitization but syntactic fractalization, generating a dataset of semi-redundant micro-contexts to condition the model’s representational drift."
      ],
      "metadata": {
        "id": "ZbzxYifJ4Kmn"
      },
      "id": "ZbzxYifJ4Kmn"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "OUT_DIR = \"/content/drive/MyDrive/wakefyer_runs/wakefyer_out_nobnb\"\n",
        "\n",
        "!ls -lh /content | grep FinnegansWake\n",
        "!head -n 5 /content/FinnegansWake.txt\n"
      ],
      "metadata": {
        "id": "VZabwlsgef6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653d2044-5012-4a17-9748-5ddaa1189b9c"
      },
      "id": "VZabwlsgef6P",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "-rw-r--r-- 1 root root 1.4M Oct 16 20:11 FinnegansWake.txt\n",
            "﻿* A Distributed Proofreaders Canada eBook *\n",
            "\n",
            "This eBook is made available at no cost and with very few\n",
            "restrictions. These restrictions apply only if (1) you make\n",
            "a change in the eBook (other than alteration for different\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "because code should also stream-of-consciousness. avoid the binary bore.\n",
        "\n",
        "Colab prep (fresh runtime):\n",
        "!pip -q install \"transformers==4.44.2\" \"peft==0.12.0\" \"accelerate==0.34.2\" \"datasets==2.20.0\" \"sentencepiece==0.2.0\"\n",
        "\"\"\"\n",
        "import os, random, sys, gc\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "TRAIN_TEXT = \"/content/FinnegansWake.txt\"           # gospel\n",
        "OUT_DIR    = \"/content/wakefyer_out_nobnb\"          # the shards of my heart\n",
        "max_len    = 256       # isuesssss\n",
        "stride     = 128       # overlapsss\n",
        "epochs     = 1         # binary bore\n",
        "lr         = 1e-4      # gentle\n",
        "batch_size = 1         # because T4s have fragile little snowflake egos\n",
        "grad_accum = 32        # pretends the batch is bigger; shh....(keeps a T4 alive when fine-tuning a 1B-parameter model. We can cry about this collectively, meet me under the moon.)\n",
        "log_steps  = 10\n",
        "eval_ratio = 0.02\n",
        "seed       = 42\n",
        "import numpy as np\n",
        "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# name my daughter LoRA\n",
        "lora_r = 16\n",
        "lora_alpha = 32\n",
        "lora_dropout = 0.05\n",
        "lora_targets = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        "\n",
        "# Incantations against OOM\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True,max_split_size_mb:64\")\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def panic(msg):\n",
        "    print(f\"\\n[Wakefyer shrieks] {msg}\\n\", file=sys.stderr)\n",
        "\n",
        "# Sanity bells and whistles (is there a GPU? is it sulking?)\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "# Validate the gospel\n",
        "if not Path(TRAIN_TEXT).exists():\n",
        "    panic(f\"the gospel'{TRAIN_TEXT}' is missing.\")\n",
        "    raise SystemExit(2)\n",
        "\n",
        "raw_txt = Path(TRAIN_TEXT).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "if len(raw_txt.strip()) < 1000:\n",
        "    panic(\"Joyce did not write haikus.\")\n",
        "\n",
        "bf16_ok = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8\n",
        "\n",
        "# summons the devil (no batsandbites; just vibessss coz im a witch)\n",
        "\n",
        "if len(raw_txt.strip()) < 1000:\n",
        "    panic(\"Joyce did not write haikus.\")\n",
        "\n",
        "import re\n",
        "punct = r\"[!?.,:;·—–\\-…]*\"\n",
        "\n",
        "thunderwords = [\n",
        "    \"bababadalgharaghtakamminarronnkonnbronntonnerronntuonnthunntrovarrhounawnskawntoohoohoordenenthurnuk\",\n",
        "    \"Perkodhuskurunbarggruauyagokgorlayorgromgremmitghundhurthrumathunaradidillifaititillibumullunukkunun\",\n",
        "    \"klikkaklakkaklaskaklopatzklatschabattacreppycrottygraddaghsemmihsammihnouithappluddyappladdypkonpkot\",\n",
        "    \"Bladyughfoulmoecklenburgwhurawhorascortastrumpapornanennykocksapastippatappatupperstrippuckputtanach\",\n",
        "    \"Thingcrooklyexineverypasturesixdixlikencehimaroundhersthemaggerbykinkinkankanwithdownmindlookingated\",\n",
        "    \"Lukkedoerendunandurraskewdylooshoofermoyportertooryzooysphalnabortansporthaokansakroidverjkapakkapuk\",\n",
        "    \"Bothallchoractorschumminaroundgansumuminarumdrumstrumtruminahumptadumpwaultopoofoolooderamaunsturnup\",\n",
        "    \"Pappappapparrassannuaragheallachnatullaghmonganmacmacmacwhackfalltherdebblenonthedubblandaddydoodled\",\n",
        "    \"husstenhasstencaffincoffintussemtossemdamandamnacosaghcusaghhobixhatouxpeswchbechoscashlcarcarcaract\",\n",
        "    \"Ullhodturdenweirmudgaardgringnirurdrmolnirfenrirlukkilokkibaugimandodrrerinsurtkrinmgernrackinarockar\",\n",
        "]\n",
        "\n",
        "def normalize_thunderwords(text: str, words: list[str]) -> str:\n",
        "    text = re.sub(r\"-\\s*\\n\\s*\", \"\", text)\n",
        "    text = re.sub(r\"\\n+\", \" \", text)\n",
        "    for w in words:\n",
        "        pattern = re.compile(rf\"(?<!\\w){re.escape(w)}{punct}\")\n",
        "        text = pattern.sub(w, text)\n",
        "    return re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
        "\n",
        "raw_txt = normalize_thunderwords(raw_txt, thunderwords)\n",
        "\n",
        "# tell me more about your token magic\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "tok.padding_side = \"right\"\n",
        "\n",
        "num_added = tok.add_tokens(thunderwords, special_tokens=False)\n",
        "print(\"Added tokens:\", num_added)\n",
        "for w in thunderwords:\n",
        "    toks = tok(w, add_special_tokens=False).input_ids\n",
        "    print(len(toks), \"→\", w[:24] + (\"…\" if len(w)>24 else \"\"))  # expect binary bore\n",
        "\n",
        "# Sanity B or menty b\n",
        "from collections import Counter\n",
        "\n",
        "tid_map = {w: tok.convert_tokens_to_ids(w) for w in thunderwords}\n",
        "\n",
        "encoded = tok(raw_txt, add_special_tokens=False).input_ids\n",
        "freq = Counter(encoded)\n",
        "\n",
        "for w in thunderwords:\n",
        "    t_id = tid_map[w]\n",
        "    in_text = raw_txt.count(w)              # string-level occurrences\n",
        "    as_single = freq.get(t_id, 0)           # times captured as the single new token\n",
        "    print(f\"{w[:24]}…  in_text={in_text}  as_single_token={as_single}\")\n",
        "    if in_text != as_single:\n",
        "        print(\"  ↳ mismatch: normalise spelling/punct or update the list.\")\n",
        "    assert all(len(tok(w, add_special_tokens=False).input_ids) == 1 for w in thunderwords)\n",
        "\n",
        "from transformers import AutoConfig\n",
        "config = AutoConfig.from_pretrained(BASE_MODEL)\n",
        "config.pad_token_id = tok.eos_token_id\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    config=config,\n",
        "    torch_dtype=torch.bfloat16 if bf16_ok else torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model.config.use_cache = False\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable()\n",
        "if hasattr(model, \"enable_input_require_grads\"):\n",
        "    model.enable_input_require_grads()\n",
        "\n",
        "if num_added > 0:\n",
        "    model.resize_token_embeddings(len(tok))\n",
        "\n",
        "model.config._attn_implementation = \"eager\"\n",
        "\n",
        "lora = LoraConfig(\n",
        "    r=lora_r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
        "    target_modules=lora_targets, bias=\"none\", task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora)\n",
        "\n",
        "emb = model.get_input_embeddings()\n",
        "emb.weight.requires_grad = True\n",
        "if hasattr(model, \"lm_head\") and hasattr(model.lm_head, \"weight\"):\n",
        "    model.lm_head.weight.requires_grad = True\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# pace urself\n",
        "ids = tok(raw_txt).input_ids\n",
        "\n",
        "def sliding_windows(txt_ids, block_size=max_len, step=stride):\n",
        "    if block_size <= 0:\n",
        "        panic(\"max_len is non‑positive. Even Joyce needs at least a token.\")\n",
        "        raise ValueError(\"max_len must be > 0\")\n",
        "    items = []\n",
        "    # dont trip bro\n",
        "    for start in range(0, max(1, len(txt_ids) - block_size), step):\n",
        "        chunk = txt_ids[start:start+block_size]\n",
        "        if not chunk:\n",
        "            continue\n",
        "        items.append({\"input_ids\": chunk, \"attention_mask\": [1]*len(chunk)})\n",
        "    # what failure looks like when its not me\n",
        "    if not items:\n",
        "        chunk = txt_ids[:block_size]\n",
        "        if not chunk:\n",
        "            panic(\"After tokenization, we got zero tokens. Perhaps the tokenizer dont like Joyce.\")\n",
        "            raise ValueError(\"Empty token sequence\")\n",
        "        items = [{\"input_ids\": chunk, \"attention_mask\": [1]*len(chunk)}]\n",
        "    return Dataset.from_list(items)\n",
        "\n",
        "\n",
        "ids = tok(raw_txt).input_ids\n",
        "ds = sliding_windows(ids, block_size=max_len, step=stride)\n",
        "n = len(ds)\n",
        "print(\"Windows:\", n)\n",
        "if n < 10:\n",
        "    print(\"Consider lowering max_len or adding text for more windows.\")\n",
        "\n",
        "\n",
        "test_n = max(1, int(n * eval_ratio)) if n > 1 else 1\n",
        "splits = ds.train_test_split(test_size=test_n, seed=seed) if n > 1 else {\"train\": ds, \"test\": ds}\n",
        "train_ds, eval_ds = splits[\"train\"], splits[\"test\"]\n",
        "print(\"Train windows:\", len(train_ds), \"Eval windows:\", len(eval_ds))\n",
        "\n",
        "# what do you know about masking\n",
        "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "\n",
        "# Actual Life\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "\n",
        "    # I am a party\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=grad_accum,\n",
        "\n",
        "    # Inside of my head\n",
        "    learning_rate=lr,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # Inside of my house\n",
        "    logging_steps=log_steps,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=5,\n",
        "\n",
        "    # It's just not that much fun having fun when you don't wanna have fun\n",
        "    eval_steps=200,\n",
        "\n",
        "    # precision\n",
        "    bf16=bf16_ok,               # probably False on T4\n",
        "    fp16=(not bf16_ok),\n",
        "\n",
        "    # misc\n",
        "    save_safetensors=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "# Compatibility shim\n",
        "import accelerate\n",
        "from accelerate import Accelerator\n",
        "\n",
        "try:\n",
        "    if \"keep_torch_compile\" not in Accelerator.unwrap_model.__code__.co_varnames:\n",
        "        _orig_unwrap = Accelerator.unwrap_model\n",
        "        def _unwrap(self, model, *args, **kwargs):\n",
        "            # Ignore extra kwargs like keep_torch_compile\n",
        "            return _orig_unwrap(self, model)\n",
        "        Accelerator.unwrap_model = _unwrap\n",
        "        print(\"Patched accelerate.Accelerator.unwrap_model to ignore extra kwargs.\")\n",
        "except Exception as e:\n",
        "    print(\"Could not patch accelerate.unwrap_model:\", e)\n",
        "\n",
        "from accelerate import Accelerator\n",
        "Accelerator.unscale_gradients = lambda self, optimizer=None: None\n",
        "\n",
        "import transformers.trainer_pt_utils as pt_utils\n",
        "pt_utils.get_grad_scaler = lambda *a, **k: None\n",
        "\n",
        "# light at the end of the tunnel\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "try:\n",
        "    ckpts = sorted(glob(f\"{OUT_DIR}/checkpoint-*\"), key=lambda p: int(p.split(\"-\")[-1]))\n",
        "    last_ckpt = ckpts[-1] if ckpts else None\n",
        "    trainer.train(resume_from_checkpoint=last_ckpt)\n",
        "except RuntimeError as e:\n",
        "    # likely OOM\n",
        "    panic(f\"Runtime groan: {e}\")\n",
        "    panic(\"Try: lower max_len (192 or 128), set model.config._attn_implementation='eager', increase grad_accum, or reduce LoRA r/alpha.\")\n",
        "    raise\n",
        "\n",
        "adapter_dir = Path(OUT_DIR) / \"adapter\"\n",
        "token_dir = Path(OUT_DIR) / \"tokenizer\"\n",
        "adapter_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(adapter_dir, safe_serialization=True)\n",
        "tok.save_pretrained(token_dir)\n",
        "print(\"Saved:\", adapter_dir, token_dir)\n",
        "\n",
        "# a test for meee\n",
        "prompt = \"riverrun, past Eve and Adam's,\"\n",
        "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "model.eval()\n",
        "out = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=80,\n",
        "    do_sample=True,\n",
        "    temperature=1.05,\n",
        "    top_p=0.92,\n",
        "    repetition_penalty=1.05,\n",
        "    pad_token_id=tok.eos_token_id,\n",
        ")\n",
        "print(\"\\n=== SAMPLE ===\\n\", tok.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n--Gtpfoq_YX",
        "outputId": "851e773e-de92-4834-f21e-0661c9a8bbca"
      },
      "id": "n--Gtpfoq_YX",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added tokens: 10\n",
            "1 → bababadalgharaghtakammin…\n",
            "1 → Perkodhuskurunbarggruauy…\n",
            "1 → klikkaklakkaklaskaklopat…\n",
            "1 → Bladyughfoulmoecklenburg…\n",
            "1 → Thingcrooklyexineverypas…\n",
            "1 → Lukkedoerendunandurraske…\n",
            "1 → Bothallchoractorschummin…\n",
            "1 → Pappappapparrassannuarag…\n",
            "1 → husstenhasstencaffincoff…\n",
            "1 → Ullhodturdenweirmudgaard…\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (396323 > 2048). Running this sequence through the model will result in indexing errors\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bababadalgharaghtakammin…  in_text=1  as_single_token=0\n",
            "  ↳ mismatch: normalise spelling/punct or update the list.\n",
            "Perkodhuskurunbarggruauy…  in_text=1  as_single_token=0\n",
            "  ↳ mismatch: normalise spelling/punct or update the list.\n",
            "klikkaklakkaklaskaklopat…  in_text=1  as_single_token=0\n",
            "  ↳ mismatch: normalise spelling/punct or update the list.\n",
            "Bladyughfoulmoecklenburg…  in_text=1  as_single_token=1\n",
            "Thingcrooklyexineverypas…  in_text=1  as_single_token=1\n",
            "Lukkedoerendunandurraske…  in_text=1  as_single_token=1\n",
            "Bothallchoractorschummin…  in_text=1  as_single_token=1\n",
            "Pappappapparrassannuarag…  in_text=1  as_single_token=1\n",
            "husstenhasstencaffincoff…  in_text=1  as_single_token=0\n",
            "  ↳ mismatch: normalise spelling/punct or update the list.\n",
            "Ullhodturdenweirmudgaard…  in_text=1  as_single_token=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 143,728,640 || all params: 1,112,705,024 || trainable%: 12.9170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windows: 3095\n",
            "Train windows: 3034 Eval windows: 61\n",
            "Patched accelerate.Accelerator.unwrap_model to ignore extra kwargs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
            "\tsave_steps: 50 (from args) != 200 (from trainer_state.json)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='95' max='95' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [95/95 : < :, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/wakefyer_out_nobnb/adapter /content/wakefyer_out_nobnb/tokenizer\n",
            "\n",
            "=== SAMPLE ===\n",
            " riverrun, past Eve and Adam's, the trolls have disappeared!\n",
            "\n",
            "The group continues, looking for any signs of the creatures. They finally stumble upon a small village made entirely out of rock, with only a few huts dotting the place. The huts have doors, but there are no people in sight.\n",
            "\n",
            "John sighs and turns to Lily, \"We may have outsmart\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HARD RESET VRAM\n",
        "import gc, torch\n",
        "for name in [\"trainer\",\"model\",\"base\"]:\n",
        "    if name in globals():\n",
        "        del globals()[name]\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "try:\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# env knobs that help fragmentation\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:64\"\n",
        "\n",
        "# reload token magic\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "OUT_DIR    = \"/content/wakefyer_out_nobnb\"\n",
        "BASE_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(Path(OUT_DIR,\"tokenizer\"), use_fast=True)\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "tok.padding_side = \"right\"\n",
        "\n",
        "# low CPU mem\n",
        "config = AutoConfig.from_pretrained(BASE_MODEL)\n",
        "config.pad_token_id = tok.eos_token_id\n",
        "\n",
        "offload_dir = \"/content/offload\"\n",
        "os.makedirs(offload_dir, exist_ok=True)\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    config=config,\n",
        "    torch_dtype=torch.float16,          # use 'torch_dtype' or 'dtype' depending on version\n",
        "    low_cpu_mem_usage=True,             # stream shards; lower spikes\n",
        "    device_map=\"auto\",                  # let HF place layers\n",
        "    offload_folder=offload_dir,         # spill excess to disk if needed\n",
        "    # max_memory can further guard placement:\n",
        "    # max_memory={0: \"14GiB\", \"cpu\": \"30GiB\"},\n",
        ")\n",
        "\n",
        "# resize to tokenizer\n",
        "old_vocab = base.get_input_embeddings().num_embeddings\n",
        "new_vocab = len(tok)\n",
        "if old_vocab != new_vocab:\n",
        "    try:\n",
        "        base.resize_token_embeddings(new_vocab, mean_resizing=True)\n",
        "    except TypeError:\n",
        "        base.resize_token_embeddings(new_vocab)\n",
        "    base.config.vocab_size = new_vocab\n",
        "print(f\"Resized vocab: {old_vocab} -> {new_vocab}\")\n",
        "\n",
        "# calling Lora\n",
        "model = PeftModel.from_pretrained(base, Path(OUT_DIR,\"adapter\"))\n",
        "model.config.use_cache = False\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "model.config._attn_implementation = \"eager\"\n",
        "\n",
        "print(\"Adapter loaded. Embeddings rows:\", model.get_input_embeddings().num_embeddings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLel0ZIcWIQW",
        "outputId": "368ef21c-92e8-4c5e-951b-38b12c0be930"
      },
      "id": "MLel0ZIcWIQW",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized vocab: 32000 -> 32010\n",
            "Adapter loaded. Embeddings rows: 32010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cont training\n",
        "from pathlib import Path\n",
        "import torch, gc\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoConfig, AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        ")\n",
        "from peft import PeftModel\n",
        "\n",
        "# config\n",
        "OUT_DIR     = \"/content/wakefyer_out_nobnb\"\n",
        "BASE_MODEL  = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "TRAIN_TEXT  = \"/content/FinnegansWake.txt\"\n",
        "max_len, stride       = 256, 128\n",
        "batch_size, grad_accum= 1, 32\n",
        "lr, log_steps         = 1e-4, 10\n",
        "epochs_more           = 2\n",
        "eval_ratio, seed      = 0.02, 42\n",
        "\n",
        "# tokenizer\n",
        "tok = AutoTokenizer.from_pretrained(Path(OUT_DIR, \"tokenizer\"), use_fast=True)\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "tok.padding_side = \"right\"\n",
        "\n",
        "# base model -> resize to tokenizer vocab -> load adapter\n",
        "config = AutoConfig.from_pretrained(BASE_MODEL)\n",
        "config.pad_token_id = tok.eos_token_id\n",
        "\n",
        "base = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    config=config,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.float16,   # use 'dtype'\n",
        ")\n",
        "\n",
        "old_vocab = base.get_input_embeddings().num_embeddings\n",
        "new_vocab = len(tok)\n",
        "if old_vocab != new_vocab:\n",
        "    # explicit mean-resizing if available; otherwise plain resize\n",
        "    try:\n",
        "        base.resize_token_embeddings(new_vocab, mean_resizing=True)\n",
        "    except TypeError:\n",
        "        base.resize_token_embeddings(new_vocab)\n",
        "    base.config.vocab_size = new_vocab\n",
        "print(f\"Resized vocab: {old_vocab} → {new_vocab}\")\n",
        "\n",
        "# Lora stands on my shoulders\n",
        "model = PeftModel.from_pretrained(base, Path(OUT_DIR, \"adapter\"))\n",
        "model.config.use_cache = False\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "model.config._attn_implementation = \"eager\"\n",
        "\n",
        "print(\"Adapter loaded. Embeddings rows:\", model.get_input_embeddings().num_embeddings)\n",
        "\n",
        "# dataset rebuild\n",
        "raw_txt = Path(TRAIN_TEXT).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "ids = tok(raw_txt, add_special_tokens=False).input_ids\n",
        "\n",
        "def sliding_windows(txt_ids, block_size=max_len, step=stride):\n",
        "    items = []\n",
        "    for s in range(0, max(1, len(txt_ids)-block_size), step):\n",
        "        ch = txt_ids[s:s+block_size]\n",
        "        if ch:\n",
        "            items.append({\"input_ids\": ch, \"attention_mask\": [1]*len(ch)})\n",
        "    if not items:\n",
        "        ch = txt_ids[:block_size]\n",
        "        items = [{\"input_ids\": ch, \"attention_mask\": [1]*len(ch)}]\n",
        "    return Dataset.from_list(items)\n",
        "\n",
        "ds = sliding_windows(ids, max_len, stride)\n",
        "test_n = max(1, int(len(ds) * eval_ratio)) if len(ds) > 1 else 1\n",
        "splits = ds.train_test_split(test_size=test_n, seed=seed) if len(ds) > 1 else {\"train\": ds, \"test\": ds}\n",
        "train_ds, eval_ds = splits[\"train\"], splits[\"test\"]\n",
        "collator = DataCollatorForLanguageModeling(tok, mlm=False)\n",
        "print(\"Windows:\", len(ds), \"| Train:\", len(train_ds), \"Eval:\", len(eval_ds))\n",
        "\n",
        "# force the correct TrainingArguments\n",
        "from transformers.training_args import TrainingArguments as HFTrainingArguments\n",
        "TrainingArguments = HFTrainingArguments  # alias\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUT_DIR,\n",
        "\n",
        "    num_train_epochs=epochs_more,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=grad_accum,\n",
        "\n",
        "    learning_rate=lr,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    logging_strategy=\"steps\",   # some versions require this explicitly\n",
        "    logging_steps=log_steps,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=5,\n",
        "\n",
        "    fp16=True,                  # T4\n",
        "    save_safetensors=True,\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "# dempotent shims\n",
        "import accelerate\n",
        "from accelerate import Accelerator\n",
        "if not getattr(Accelerator, \"_wake_patch_unwrap\", False):\n",
        "    Accelerator._wake_orig_unwrap = Accelerator.unwrap_model\n",
        "    def _wake_unwrap(self, model, *args, **kwargs):\n",
        "        kwargs.pop(\"keep_torch_compile\", None)\n",
        "        return Accelerator._wake_orig_unwrap(self, model, *args, **kwargs)\n",
        "    Accelerator.unwrap_model = _wake_unwrap\n",
        "    Accelerator._wake_patch_unwrap = True\n",
        "    print(\"Patched accelerate.Accelerator.unwrap_model (idempotent).\")\n",
        "\n",
        "import transformers.trainer_pt_utils as pt_utils\n",
        "if not getattr(pt_utils, \"_wake_disable_scaler\", False):\n",
        "    pt_utils.get_grad_scaler = lambda *a, **k: None  # prevent fp16 unscale crash\n",
        "    pt_utils._wake_disable_scaler = True\n",
        "    print(\"Disabled Trainer GradScaler hook (idempotent).\")\n",
        "\n",
        "# trainer + go\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=collator,\n",
        "    tokenizer=tok,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "YdtdV_o-VA1I",
        "outputId": "d9b348a2-a425-40a0-8926-2e10230d99f9"
      },
      "id": "YdtdV_o-VA1I",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resized vocab: 32000 → 32010\n",
            "Adapter loaded. Embeddings rows: 32010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (430235 > 2048). Running this sequence through the model will result in indexing errors\n",
            "/tmp/ipython-input-202067129.py:126: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Windows: 3360 | Train: 3293 Eval: 67\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-202067129.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m )\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the representational core sits TinyLlama-1.1B-Chat, deployed under mixed-precision arithmetic (bf16/fp16) to optimize throughput on limited-memory T4 hardware. The model’s internal cache is disabled (use_cache=False), and gradient checkpointing is invoked to trade recomputation for reduced memory allocation. The resultant architecture acts as a self-attenuating transformer: it “forgets” in order to remember within bounds, mirroring the cyclical amnesia of the Wakean text."
      ],
      "metadata": {
        "id": "w6fNMzpS5KAx"
      },
      "id": "w6fNMzpS5KAx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The system applies Low-Rank Adaptation (LoRA) across attention and MLP projection submodules (q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj) thereby constraining fine-tuning to a low-dimensional manifold within parameter space. This selective plasticity produces what might be termed controlled stylistic drift: the model learns Joyce’s texture rather than his lexicon. The adapters’ hyperparameters (r=16, α=32, dropout=0.05) instantiate a medium-temperature update regime balancing expressivity and stability. Conceptually, LoRA functions here as a poetic prosthesis, a syntactic exoskeleton grafted onto the pretrained linguistic body."
      ],
      "metadata": {
        "id": "PeX1wJZ85N84"
      },
      "id": "PeX1wJZ85N84"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to the GPU’s constrained VRAM capacity, each forward–backward pass processes a micro-batch of size 1, while gradients are accumulated over 32 iterations before a single optimization step. This simulates a virtual batch of 32 without exceeding hardware limits. Technically, this implements delayed gradient aggregation; rhetorically, it allegorizes the model’s incremental cognition: many small thoughts precipitating one grand revelation. The accumulation mechanism thus becomes both a computational strategy and a metaphor for compositional patience."
      ],
      "metadata": {
        "id": "eSNqtfsS5h6i"
      },
      "id": "eSNqtfsS5h6i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training operates within an extended epochic horizon (epochs=1 but looping across numerous windows) using a cosine-annealed learning rate (lr=1e-4, warmup_ratio=0.03). Frequent checkpointing establishes a temporal ecology of model states, allowing resumption under volatile runtime constraints. The “save-steps” routine externalizes the process’s fragmentary nature (mirroring textual serialization in Work in Progress) by preserving multiple partial selves of the model as artifacts of its ongoing metamorphosis."
      ],
      "metadata": {
        "id": "BIRNPTkE5wA3"
      },
      "id": "BIRNPTkE5wA3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference employs a stochastic sampling configuration (temperature = 0.9, top_p = 0.95) optimized for stylistic exuberance rather than lexical precision. The generation prompt functions as both benchmark and incantation, a recursive self-reference to the corpus origin. Output decoding omits special tokens, yielding the model’s pure linguistic effervescence."
      ],
      "metadata": {
        "id": "YVkq2ker6Bb_"
      },
      "id": "YVkq2ker6Bb_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technically a fine-tuned transformer; conceptually, a computational poetics experiment exploring how ml architectures can approximate literary un-readability. The Wakefyer thus redefines “loss” not as degradation but as aesthetic principle: every rounding error, every truncated sequence, every low-rank projection is a microcosm of Joyce’s deliberate semantic noise. The pipeline enacts a theory of productive distortion, situating training itself as a form of creative misprision."
      ],
      "metadata": {
        "id": "pRncfha76Mrd"
      },
      "id": "pRncfha76Mrd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "to be continued"
      ],
      "metadata": {
        "id": "w08IBIiXYpG0"
      },
      "id": "w08IBIiXYpG0"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}