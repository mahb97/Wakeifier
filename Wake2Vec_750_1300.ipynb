{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahb97/Wakeifier/blob/main/Wake2Vec_750_1300.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wake2Vec: Lexicon-Augmented Embedding Training (Steps 750-1300)\n",
        "\n",
        "Continuation training for embedding-only fine-tuning of TinyLlama-1.1B with Finnegans Wake lexicon injection.\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a continuation phase of embedding-only fine-tuning for a large language model augmented with approximately 44,000 Wake-specific tokens. The training methodology isolates new token embeddings through gradient masking while keeping pre-trained model parameters frozen, enabling lexical integration into the existing semantic space without catastrophic forgetting.\n",
        "\n",
        "## Training Configuration\n",
        "\n",
        "Phase: Continuation from checkpoint 750 to step 1300\n",
        "\n",
        "Model: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\n",
        "\n",
        "Training strategy: Embedding-only optimization with gradient masking on base vocabulary\n",
        "\n",
        "Data: Finnegans Wake corpus with held-out validation set\n",
        "\n",
        "Optimization: Adafactor (learning rate 5e-4, no warmup)\n",
        "\n",
        "Hardware: Single T4 GPU (15GB VRAM) via Google Colab\n",
        "\n",
        "## Notebook Structure\n",
        "\n",
        "This notebook is organized into six sequential cells for reproducible execution:\n",
        "\n",
        "1. Path configuration and environment setup\n",
        "2. Dependency pinning and compatibility patches\n",
        "3. Model and tokenizer loading with gradient masking\n",
        "4. Dataset loading and sequence truncation\n",
        "5. Training callback definitions (monitoring, backups, snapshots)\n",
        "6. Training execution and resumption logic\n",
        "\n",
        "## Monitoring and Backup Utilities\n",
        "\n",
        "The notebook provides automated systems for long-running training stability:\n",
        "\n",
        "- Loss and evaluation metric tracking via structured JSON logs\n",
        "- Checkpoint inventory and validation (weights, optimizer state, trainer state)\n",
        "- Sentry mirror system for automated checkpoint backups to Google Drive\n",
        "- Embedding snapshot capture at 50-step intervals\n",
        "- Training throughput diagnostics (steps per second reporting)\n",
        "- Heartbeat metadata for remote monitoring\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "The training resumes from checkpoint-750 and proceeds with the following hyperparameters:\n",
        "\n",
        "- Batch size: 1 (effective batch size 16 via gradient accumulation)\n",
        "- Gradient accumulation steps: 16\n",
        "- Learning rate: 5e-4 (no warmup)\n",
        "- Sequence length: 384 tokens (runtime truncation)\n",
        "- Save frequency: 75 steps\n",
        "- Evaluation frequency: 200 steps\n",
        "- Checkpoint retention: 3 most recent (memory optimization)\n",
        "- Gradient clipping: maximum norm 1.0\n",
        "- Gradient checkpointing: enabled for memory efficiency\n",
        "\n",
        "All training callbacks (evaluation triggers, sentry mirroring, embedding snapshots, throughput monitoring) are integrated into the Hugging Face Trainer workflow for automated execution during training. The notebook includes compatibility patches for transformers 4.57.1 and accelerate 1.2.1 to ensure stable execution on Colab environments."
      ],
      "metadata": {
        "id": "CIBlQSpl0iaU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a2ct7YLfVT5",
        "outputId": "faf6a7b1-8f2c-49b4-d772-34bf3e602514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t4_1762376560 step 550 loss 3.2604\n"
          ]
        }
      ],
      "source": [
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "run = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "log = json.loads((run/\"metrics\"/\"phase1_loss_log.json\").read_text())\n",
        "print(run.name, \"step\", log[-1][\"step\"], \"loss\", round(float(log[-1][\"loss\"]),4))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "run = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "ck = sorted(run.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)[0]\n",
        "state = json.loads((ck/\"trainer_state.json\").read_text())\n",
        "ev = [d for d in state.get(\"log_history\", []) if \"eval_loss\" in d]\n",
        "print(ev[-1] if ev else \"no eval yet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWBAGHBE05lD",
        "outputId": "c5db6825-ef90-4537-e8b4-5e2d799b4fac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 10.527472527472527, 'eval_loss': 7.096441268920898, 'eval_runtime': 13.6439, 'eval_samples_per_second': 3.518, 'eval_steps_per_second': 0.44, 'step': 600}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "snapshot"
      ],
      "metadata": {
        "id": "llry5ube8_-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual snapshot\n",
        "import pathlib, shutil, time\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN   = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "def latest_ckpt_with_weights(base):\n",
        "    cands = sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cands:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "src = latest_ckpt_with_weights(RUN)\n",
        "assert src is not None, \"No valid checkpoint with weights found yet.\"\n",
        "SNAPS = RUN/\"snapshots\"; SNAPS.mkdir(exist_ok=True, parents=True)\n",
        "dst = SNAPS/f\"snap_{int(time.time())}_{src.name}\"\n",
        "if not dst.exists():\n",
        "    shutil.copytree(src, dst)\n",
        "print(\"[SNAP] Saved\", dst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqEsKdkn8_R8",
        "outputId": "6653c150-b5f6-4d7d-c969-76b50ed2daa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SNAP] Saved /content/drive/MyDrive/wake2vec/runs/t4_1762376560/snapshots/snap_1762565503_checkpoint-300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentry mirror"
      ],
      "metadata": {
        "id": "61EtZyYu9GZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# newest full checkpoint + metrics\n",
        "import pathlib, shutil\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN   = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "SENTRY = DRIVE/\"sentry_backups\"/RUN.name; SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def latest_ckpt_with_weights(base):\n",
        "    cands = sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cands:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "ck = latest_ckpt_with_weights(RUN)\n",
        "if ck is None:\n",
        "    print(\"[SENTRY] No full checkpoint yet.\")\n",
        "else:\n",
        "    dst = SENTRY/ck.name\n",
        "    if not dst.exists():\n",
        "        shutil.copytree(ck, dst)\n",
        "        print(f\"[SENTRY] Mirrored {ck.name} → {dst}\")\n",
        "    else:\n",
        "        print(\"[SENTRY] Already mirrored:\", dst)\n",
        "\n",
        "# mirror metrics JSONs\n",
        "mdst = SENTRY/\"metrics\"; mdst.mkdir(parents=True, exist_ok=True)\n",
        "for f in (RUN/\"metrics\").glob(\"*.json\"):\n",
        "    shutil.copy2(f, mdst/f.name)\n",
        "print(\"[SENTRY] Metrics mirrored →\", mdst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ikqpbn0P9IZI",
        "outputId": "134a49e7-7430-468b-82b6-248f58c83e7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SENTRY] Mirrored checkpoint-300 → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/checkpoint-300\n",
            "[SENTRY] Metrics mirrored → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/metrics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss and checkpoint"
      ],
      "metadata": {
        "id": "CzGNXVVp9jbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heartbeat\n",
        "import json, pathlib\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN   = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "\n",
        "# loss\n",
        "mlog = RUN/\"metrics\"/\"phase1_loss_log.json\"\n",
        "if mlog.exists():\n",
        "    logs = json.loads(mlog.read_text())\n",
        "    print(f\"[LOSS] step={logs[-1]['step']}  loss={float(logs[-1]['loss']):.4f}\")\n",
        "else:\n",
        "    print(\"[LOSS] no metrics yet\")\n",
        "\n",
        "# checkpoints\n",
        "def scan(base):\n",
        "    rows=[]\n",
        "    for ck in sorted(base.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1])):\n",
        "        s=int(ck.name.split(\"-\")[-1])\n",
        "        rows.append((\n",
        "            s,\n",
        "            (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists(),\n",
        "            (ck/\"trainer_state.json\").exists(),\n",
        "            (ck/\"optimizer.pt\").exists(),\n",
        "        ))\n",
        "    return rows\n",
        "\n",
        "print(\"\\n[RUNS]\")\n",
        "for s,w,t,o in scan(RUN):\n",
        "    print(f\"  {s:>4}  weights={str(w):5}  state={str(t):5}  opt={str(o):5}\")\n",
        "\n",
        "SENTRY = DRIVE/\"sentry_backups\"/RUN.name\n",
        "if SENTRY.exists():\n",
        "    print(\"\\n[SENTRY]\")\n",
        "    for s,w,t,o in scan(SENTRY):\n",
        "        print(f\"  {s:>4}  weights={str(w):5}  state={str(t):5}  opt={str(o):5}\")\n",
        "else:\n",
        "    print(\"\\n[SENTRY] none\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjpiKxqN9mFs",
        "outputId": "5c78aab1-bdc2-43c9-a0e1-ba56cadd382a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LOSS] step=550  loss=3.2604\n",
            "\n",
            "[RUNS]\n",
            "   100  weights=True   state=True   opt=True \n",
            "   200  weights=True   state=True   opt=True \n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=True   opt=True \n",
            "   700  weights=False  state=True   opt=True \n",
            "\n",
            "[SENTRY]\n",
            "   300  weights=True   state=True   opt=True \n",
            "   400  weights=False  state=True   opt=True \n",
            "   500  weights=False  state=True   opt=True \n",
            "   600  weights=False  state=False  opt=False\n",
            "   700  weights=False  state=True   opt=True \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "clean drive"
      ],
      "metadata": {
        "id": "CqniUpma9tAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Light flush to help sync small files\n",
        "import os, time, pathlib\n",
        "RUN = max((pathlib.Path(\"/content/drive/MyDrive/wake2vec\")/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "(open(RUN/\"_touch.sync\",\"w\")).write(str(time.time()))\n",
        "os.sync()\n",
        "print(\"[SYNC] touched + sync hinted\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CW2IWyO9uUH",
        "outputId": "eab2fc1d-3869-4f53-d45f-912aae37715b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SYNC] touched + sync hinted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save if trainer is in scope"
      ],
      "metadata": {
        "id": "f-RvQNxm92Jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    trainer.save_model()\n",
        "    if hasattr(trainer, \"_save_checkpoint\"):\n",
        "        trainer._save_checkpoint(model=trainer.model, trial=None)\n",
        "    print(\"[TRAINER] save requested\")\n",
        "except NameError:\n",
        "    print(\"[TRAINER] No 'trainer' object in this notebook; use the mirror cell instead.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwlnYfrg-MEZ",
        "outputId": "b060273e-f977-4641-ebaf-2750fe52d87e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAINER] No 'trainer' object in this notebook; use the mirror cell instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "print(\"Drive mounted.\")"
      ],
      "metadata": {
        "id": "VuVTCDxICx2A",
        "outputId": "43839b01-93c6-42fa-d07b-44d68a1bb5dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VERBOSE mirror of latest full checkpoint (+ metrics) to sentry_backups\n",
        "import pathlib, shutil, time, os\n",
        "\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "CANDIDATES = [pathlib.Path(\"/content/runs\"), DRIVE/\"runs\"]\n",
        "\n",
        "def latest_run():\n",
        "    runs = []\n",
        "    for root in CANDIDATES:\n",
        "        if root.exists():\n",
        "            for p in root.glob(\"t4_*\"):\n",
        "                try:\n",
        "                    runs.append((p.stat().st_mtime, p))\n",
        "                except FileNotFoundError:\n",
        "                    pass\n",
        "    if not runs: return None\n",
        "    runs.sort(reverse=True)\n",
        "    return runs[0][1]\n",
        "\n",
        "def latest_ckpt_with_weights(run):\n",
        "    if not run: return None\n",
        "    cks = sorted(run.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "    for ck in cks:\n",
        "        if (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists():\n",
        "            return ck\n",
        "    return None\n",
        "\n",
        "RUN = latest_run()\n",
        "print(\"[INFO] Active run:\", RUN if RUN else \"none\")\n",
        "CK  = latest_ckpt_with_weights(RUN)\n",
        "print(\"[INFO] Latest full ckpt:\", CK if CK else \"none\")\n",
        "\n",
        "if CK is None:\n",
        "    print(\"[SENTRY] No checkpoint with weights yet — will retry after next save.\")\n",
        "else:\n",
        "    SENTRY = DRIVE/\"sentry_backups\"/RUN.name\n",
        "    SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "    DST = SENTRY/CK.name\n",
        "\n",
        "    src_mtime = time.ctime(CK.stat().st_mtime)\n",
        "    print(f\"[SENTRY] Source: {CK} (mtime {src_mtime})\")\n",
        "    if DST.exists():\n",
        "        # check if dest is older/stale by file count or mtime\n",
        "        src_files = sum(1 for _ in CK.rglob(\"*\"))\n",
        "        dst_files = sum(1 for _ in DST.rglob(\"*\"))\n",
        "        print(f\"[SENTRY] Already exists: {DST} (files src={src_files} dst={dst_files})\")\n",
        "        if dst_files < src_files:\n",
        "            print(\"[SENTRY] Detected partial mirror; refreshing…\")\n",
        "            shutil.rmtree(DST)\n",
        "            shutil.copytree(CK, DST)\n",
        "            print(\"[SENTRY] Re-mirrored:\", DST)\n",
        "        else:\n",
        "            print(\"[SENTRY] Mirror up-to-date.\")\n",
        "    else:\n",
        "        shutil.copytree(CK, DST)\n",
        "        print(\"[SENTRY] Mirrored:\", DST)\n",
        "\n",
        "    # Mirror metrics verbosely\n",
        "    msrc = RUN/\"metrics\"\n",
        "    mdst = SENTRY/\"metrics\"\n",
        "    mdst.mkdir(parents=True, exist_ok=True)\n",
        "    copied = 0\n",
        "    if msrc.exists():\n",
        "        for f in msrc.glob(\"*.json\"):\n",
        "            shutil.copy2(f, mdst/f.name)\n",
        "            copied += 1\n",
        "    print(f\"[SENTRY] Metrics mirrored → {mdst} ({copied} files)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJdM-MCcJoVP",
        "outputId": "8122a071-095e-4bb2-96ed-4f51e0f3c907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Active run: /content/drive/MyDrive/wake2vec/runs/t4_1762376560\n",
            "[INFO] Latest full ckpt: /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300\n",
            "[SENTRY] Source: /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300 (mtime Wed Nov  5 22:45:46 2025)\n",
            "[SENTRY] Already exists: /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/checkpoint-300 (files src=12 dst=12)\n",
            "[SENTRY] Mirror up-to-date.\n",
            "[SENTRY] Metrics mirrored → /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560/metrics (1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "BASE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN = BASE/\"runs\"/\"t4_1762376560\"  # adjust if different\n",
        "SENTRY = BASE/\"sentry_backups\"/\"t4_1762376560\"\n",
        "\n",
        "def audit(root):\n",
        "    print(f\"\\n[{root}]\")\n",
        "    for ck in sorted(root.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1])):\n",
        "        step = int(ck.name.split(\"-\")[-1])\n",
        "        w = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "        t = (ck/\"trainer_state.json\").exists()\n",
        "        o = (ck/\"optimizer.pt\").exists()\n",
        "        print(f\"{step:>5}  weights={w:<5}  state={t:<5}  opt={o:<5}  → {ck.name}\")\n",
        "\n",
        "if RUN.exists():   audit(RUN)\n",
        "if SENTRY.exists(): audit(SENTRY)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLwaBc-GKRK8",
        "outputId": "9c33c77a-c5bf-49ba-f91c-130eb5454270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[/content/drive/MyDrive/wake2vec/runs/t4_1762376560]\n",
            "  100  weights=1      state=1      opt=1      → checkpoint-100\n",
            "  200  weights=1      state=1      opt=1      → checkpoint-200\n",
            "  300  weights=1      state=1      opt=1      → checkpoint-300\n",
            "  400  weights=0      state=1      opt=1      → checkpoint-400\n",
            "  500  weights=0      state=1      opt=1      → checkpoint-500\n",
            "  600  weights=0      state=1      opt=1      → checkpoint-600\n",
            "  700  weights=0      state=1      opt=1      → checkpoint-700\n",
            "\n",
            "[/content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560]\n",
            "  300  weights=1      state=1      opt=1      → checkpoint-300\n",
            "  400  weights=0      state=1      opt=1      → checkpoint-400\n",
            "  500  weights=0      state=1      opt=1      → checkpoint-500\n",
            "  600  weights=0      state=0      opt=0      → checkpoint-600\n",
            "  700  weights=0      state=1      opt=1      → checkpoint-700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "roll it from 750"
      ],
      "metadata": {
        "id": "5-M1trtPOdoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Path config\n",
        "DRIVE = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "DATASETS = Path(\"/content/datasets\")\n",
        "LOCAL_RUN = Path(\"/content/runs\") / RUN_ID\n",
        "SENTRY = DRIVE / \"sentry_backups\" / RUN_ID\n",
        "RESUME_FROM = DRIVE / \"runs\" / RUN_ID / \"checkpoint-750-rebuilt\"\n",
        "\n",
        "# directories\n",
        "LOCAL_RUN.mkdir(parents=True, exist_ok=True)\n",
        "SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Training config\n",
        "FINAL_TARGET = 1300\n",
        "LAST_STEP = 750\n",
        "TARGET = min(FINAL_TARGET, LAST_STEP + 550)\n",
        "\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "\n",
        "print(\"[PATH CONFIGURATION]\")\n",
        "print(f\"  RUN_ID: {RUN_ID}\")\n",
        "print(f\"  RESUME_FROM: {RESUME_FROM}\")\n",
        "print(f\"  LOCAL_RUN: {LOCAL_RUN}\")\n",
        "print(f\"  SENTRY: {SENTRY}\")\n",
        "print(f\"  DATASETS: {DATASETS}\")\n",
        "print(f\"  TARGET: {LAST_STEP} → {TARGET}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "273hKq1e3yk1",
        "outputId": "e9663700-6dfb-46a4-dcb5-ccf7078877ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[PATH CONFIGURATION]\n",
            "  RUN_ID: t4_1762376560\n",
            "  RESUME_FROM: /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\n",
            "  LOCAL_RUN: /content/runs/t4_1762376560\n",
            "  SENTRY: /content/drive/MyDrive/wake2vec/sentry_backups/t4_1762376560\n",
            "  DATASETS: /content/datasets\n",
            "  TARGET: 750 → 1300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependency Pin & Compat Patches\n",
        "import sys\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def pin(pkg, ver):\n",
        "    \"\"\"Pin package to specific version\"\"\"\n",
        "    try:\n",
        "        m = importlib.import_module(pkg)\n",
        "        assert m.__version__ == ver\n",
        "        print(f\"[OK] {pkg} {m.__version__}\")\n",
        "    except Exception:\n",
        "        print(f\"[PIN] {pkg}=={ver}\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{pkg}=={ver}\", \"-q\"])\n",
        "        m = importlib.import_module(pkg)\n",
        "        print(f\"[OK] {pkg} {m.__version__}\")\n",
        "\n",
        "pin(\"transformers\", \"4.57.1\")\n",
        "pin(\"accelerate\", \"1.2.1\")\n",
        "pin(\"datasets\", \"2.21.0\")\n",
        "\n",
        "# Apply unwrap_model compatibility patch\n",
        "import accelerate\n",
        "if not hasattr(accelerate.Accelerator, \"_w2v_patched\"):\n",
        "    _orig = accelerate.Accelerator.unwrap_model\n",
        "    def _shim(self, model, *args, **kw):\n",
        "        kw.pop(\"keep_torch_compile\", None)\n",
        "        return _orig(self, model, *args, **kw)\n",
        "    accelerate.Accelerator.unwrap_model = _shim\n",
        "    accelerate.Accelerator._w2v_patched = True\n",
        "    print(\"[PATCH] unwrap_model compatibility shim active\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8KoalXX38Yv",
        "outputId": "568198a5-07d0-4990-997f-6515f6da752a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] transformers 4.57.1\n",
            "[OK] accelerate 1.2.1\n",
            "[OK] datasets 2.21.0\n",
            "[PATCH] unwrap_model compatibility shim active\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch, os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb=64\"\n",
        "\n",
        "for k in [\"trainer\",\"model\",\"tok\",\"optimizer\",\"scheduler\",\"dc\",\"train_ds\",\"valid_ds\"]:\n",
        "    if k in globals():\n",
        "        try: del globals()[k]\n",
        "        except: pass\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "try: torch.cuda.ipc_collect()\n",
        "except: pass"
      ],
      "metadata": {
        "id": "fv8bq5WsDSbB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and Tok\n",
        "import torch\n",
        "import gc\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Clear GPU\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Load tok\n",
        "tok = AutoTokenizer.from_pretrained(str(RESUME_FROM), use_fast=True)\n",
        "if tok.pad_token_id is None:\n",
        "    tok.pad_token = tok.eos_token or \"</s>\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    str(RESUME_FROM),\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Config model\n",
        "model.config.use_cache = False\n",
        "model.config.attn_implementation = \"eager\"\n",
        "\n",
        "# freeze all except embeddings\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "emb = model.get_input_embeddings()\n",
        "emb.weight.requires_grad = True\n",
        "\n",
        "# Tie output head to input embeds\n",
        "with torch.no_grad():\n",
        "    model.get_output_embeddings().weight = emb.weight\n",
        "\n",
        "model.train()\n",
        "\n",
        "# Verification\n",
        "print(\"[MODEL LOADED]\")\n",
        "print(f\"  Tied weights: {'OK' if emb.weight.data_ptr() == model.get_output_embeddings().weight.data_ptr() else 'NO'}\")\n",
        "print(f\"  Embedding trainable: {emb.weight.requires_grad}\")\n",
        "print(f\"  Pad token ID: {tok.pad_token_id}\")\n",
        "\n",
        "import collections\n",
        "devs = collections.Counter(p.device.type for p in model.parameters())\n",
        "print(f\"  Device distribution: {dict(devs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv267cQA4Ja-",
        "outputId": "bfcb3fdd-604a-4b40-f547-917d2d3a48a5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MODEL LOADED]\n",
            "  Tied weights: OK\n",
            "  Embedding trainable: True\n",
            "  Pad token ID: 2\n",
            "  Device distribution: {'cuda': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "import json\n",
        "import time\n",
        "import shutil\n",
        "from transformers import TrainerCallback\n",
        "from transformers.trainer_callback import TrainerState\n",
        "\n",
        "class EvalEveryNSteps(TrainerCallback):\n",
        "    \"\"\"Trigger evaluation at fixed step intervals\"\"\"\n",
        "    def __init__(self, n=200):\n",
        "        self.n = n\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s = int(state.global_step or 0)\n",
        "        if s and s % self.n == 0:\n",
        "            control.should_log = True\n",
        "            control.should_evaluate = True\n",
        "\n",
        "class SentryMirror(TrainerCallback):\n",
        "    \"\"\"Mirror checkpoints to backup directory on Drive\"\"\"\n",
        "    def on_save(self, args, state, control, **kw):\n",
        "        try:\n",
        "            out = Path(args.output_dir)\n",
        "            cks = [p for p in out.glob(\"checkpoint-*\") if p.is_dir()]\n",
        "            if not cks:\n",
        "                return\n",
        "\n",
        "            def step_of(p):\n",
        "                try:\n",
        "                    return int(p.name.split(\"-\")[-1])\n",
        "                except:\n",
        "                    return -1\n",
        "\n",
        "            ck = max(cks, key=step_of)\n",
        "            has_weights = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "\n",
        "            if not has_weights:\n",
        "                return\n",
        "\n",
        "            dst = SENTRY / ck.name\n",
        "            if not dst.exists():\n",
        "                shutil.copytree(ck, dst)\n",
        "                print(f\"[SENTRY] mirrored {ck.name}\")\n",
        "\n",
        "            # Mirror metrics\n",
        "            msrc = out / \"metrics\"\n",
        "            if msrc.exists():\n",
        "                (SENTRY/\"metrics\").mkdir(parents=True, exist_ok=True)\n",
        "                for f in msrc.glob(\"*.json\"):\n",
        "                    shutil.copy2(f, SENTRY/\"metrics\"/f.name)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"[SENTRY] mirror failed: {e}\")\n",
        "\n",
        "class EmbeddingSnap(TrainerCallback):\n",
        "    \"\"\"Save embedding snapshots at regular intervals\"\"\"\n",
        "    def __init__(self, every=50):\n",
        "        self.every = every\n",
        "        (DRIVE/\"emb_snaps\"/RUN_ID).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s = int(state.global_step or 0)\n",
        "        if s and s % self.every == 0:\n",
        "            try:\n",
        "                E = model.get_input_embeddings().weight.detach().cpu()\n",
        "                out_path = (DRIVE/\"emb_snaps\"/RUN_ID) / f\"emb_step{s:04d}.pt\"\n",
        "                torch.save(E, out_path)\n",
        "\n",
        "                # Write heartbeat metadata\n",
        "                heartbeat = {\n",
        "                    \"run_id\": RUN_ID,\n",
        "                    \"step\": s,\n",
        "                    \"rows\": int(E.size(0)),\n",
        "                    \"dim\": int(E.size(1)),\n",
        "                    \"ts\": time.time()\n",
        "                }\n",
        "                (out_path.parent/\"heartbeat.json\").write_text(\n",
        "                    json.dumps(heartbeat, indent=2))\n",
        "\n",
        "                print(f\"[SNAP] embeddings saved to {out_path.name}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"[SNAP] failed: {e}\")\n",
        "\n",
        "class StepTimer(TrainerCallback):\n",
        "    \"\"\"Monitor training throughput in steps per second\"\"\"\n",
        "    def __init__(self, every=10):\n",
        "        self.prev = None\n",
        "        self.t = None\n",
        "        self.every = every\n",
        "\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s = int(state.global_step or 0)\n",
        "        now = time.time()\n",
        "\n",
        "        if self.prev is not None and s > self.prev and s % self.every == 0:\n",
        "            dt = now - self.t\n",
        "            print(f\"[{s:4d}] ~{dt/self.every:.2f}s/step (last {self.every})\")\n",
        "\n",
        "        self.prev, self.t = s, now\n",
        "\n",
        "print(\"[CALLBACKS CONFIGURED]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VpyEAhM5BFk",
        "outputId": "a399b6d1-096e-499a-aed0-4ded29d6d75f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CALLBACKS CONFIGURED]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pin and patch FIRST\n",
        "import sys, subprocess, importlib\n",
        "\n",
        "def pin(pkg, ver):\n",
        "    try:\n",
        "        m = importlib.import_module(pkg)\n",
        "        assert m.__version__ == ver\n",
        "        print(f\"[OK] {pkg} {m.__version__}\")\n",
        "    except Exception:\n",
        "        print(f\"[PIN] {pkg}=={ver}\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{pkg}=={ver}\", \"-q\"])\n",
        "        m = importlib.import_module(pkg)\n",
        "        print(f\"[OK] {pkg} {m.__version__}\")\n",
        "\n",
        "pin(\"transformers\", \"4.57.1\")\n",
        "pin(\"accelerate\", \"1.2.1\")\n",
        "pin(\"datasets\", \"2.21.0\")\n",
        "\n",
        "import accelerate\n",
        "if not hasattr(accelerate.Accelerator, \"_w2v_patched\"):\n",
        "    _orig = accelerate.Accelerator.unwrap_model\n",
        "    def _shim(self, model, *args, **kw):\n",
        "        kw.pop(\"keep_torch_compile\", None)\n",
        "        return _orig(self, model, *args, **kw)\n",
        "    accelerate.Accelerator.unwrap_model = _shim\n",
        "    accelerate.Accelerator._w2v_patched = True\n",
        "    print(\"[PATCH] active\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvIgdVTFGVCc",
        "outputId": "e4dcdd70-4329-4308-d386-7adf768c1a0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] transformers 4.57.1\n",
            "[OK] accelerate 1.2.1\n",
            "[OK] datasets 2.21.0\n",
            "[PATCH] active\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASETS\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "# mount is idempotent in Colab\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "DRIVE = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "DATASETS = Path(\"/content/datasets\"); DATASETS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def ensure_dir(src: Path, dst: Path):\n",
        "    assert src.exists(), f\"Missing dataset at {src}\"\n",
        "    if not dst.exists():\n",
        "        print(f\"[DATA] copying {src} → {dst}\")\n",
        "        shutil.copytree(src, dst)\n",
        "    else:\n",
        "        print(f\"[DATA] already local:\", dst)\n",
        "\n",
        "# try to copy to local (fast path)\n",
        "src_train = DRIVE/\"datasets\"/\"train_ds\"\n",
        "src_valid = DRIVE/\"datasets\"/\"valid_ds\"\n",
        "dst_train = DATASETS/\"train_ds\"\n",
        "dst_valid = DATASETS/\"valid_ds\"\n",
        "ensure_dir(src_train, dst_train)\n",
        "ensure_dir(src_valid, dst_valid)\n",
        "\n",
        "# load with a Drive fallback just in case\n",
        "try:\n",
        "    train_ds_path = str(dst_train if dst_train.exists() else src_train)\n",
        "    valid_ds_path = str(dst_valid if dst_valid.exists() else src_valid)\n",
        "    print(\"[DATA] train_ds:\", train_ds_path)\n",
        "    print(\"[DATA] valid_ds:\", valid_ds_path)\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Dataset path resolution failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF7tooxRCCe6",
        "outputId": "6af61578-2e17-49c8-d167-e1f843759f0d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[DATA] already local: /content/datasets/train_ds\n",
            "[DATA] already local: /content/datasets/valid_ds\n",
            "[DATA] train_ds: /content/datasets/train_ds\n",
            "[DATA] valid_ds: /content/datasets/valid_ds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIMIZED 750-1300 (Memory-Safe for T4, with grad checkpointing)\n",
        "import os, gc, json, time, shutil, collections, torch\n",
        "from pathlib import Path\n",
        "from datasets import load_from_disk\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments,\n",
        "                          Trainer, TrainerCallback)\n",
        "from transformers.trainer_callback import TrainerState\n",
        "\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# Paths\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "RESUME_FROM = Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\")\n",
        "DRIVE = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "LOCAL_RUN = Path(\"/content/runs\")/RUN_ID\n",
        "LOCAL_RUN.mkdir(parents=True, exist_ok=True)\n",
        "DATASETS = Path(\"/content/datasets\")\n",
        "train_ds_path = DATASETS / \"train_ds\"\n",
        "valid_ds_path = DATASETS / \"valid_ds\"\n",
        "\n",
        "# Memory optimization\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Load tokenizer\n",
        "tok = AutoTokenizer.from_pretrained(str(RESUME_FROM), use_fast=True)\n",
        "if tok.pad_token_id is None:\n",
        "    tok.pad_token = tok.eos_token or \"</s>\"\n",
        "\n",
        "# Load model with device_map auto\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    str(RESUME_FROM),\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.attn_implementation = \"eager\"  # Safer than SDPA\n",
        "\n",
        "# Freeze and tie\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "emb = model.get_input_embeddings()\n",
        "emb.weight.requires_grad = True\n",
        "with torch.no_grad():\n",
        "    model.get_output_embeddings().weight = emb.weight\n",
        "\n",
        "model.train()\n",
        "print(\"[TIED]\", \"OK\" if emb.weight.data_ptr()==model.get_output_embeddings().weight.data_ptr() else \"NO\")\n",
        "\n",
        "# Load data with smaller valid set\n",
        "train_ds = load_from_disk(str(train_ds_path))\n",
        "valid_all = load_from_disk(str(valid_ds_path))\n",
        "valid_ds = valid_all.select(range(min(300, len(valid_all))))  # Reduced from 1000\n",
        "\n",
        "# Truncating collator\n",
        "base_dc = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "MAX_LEN = 320\n",
        "\n",
        "class TruncatingCollator:\n",
        "    def __init__(self, base, max_len=MAX_LEN):\n",
        "        self.base, self.max_len = base, max_len\n",
        "    def __call__(self, feats):\n",
        "        out = self.base(feats)\n",
        "        for k,v in list(out.items()):\n",
        "            if isinstance(v, torch.Tensor) and v.dim()==2:\n",
        "                out[k] = v[:, :self.max_len]\n",
        "        return out\n",
        "\n",
        "dc = TruncatingCollator(base_dc, MAX_LEN)\n",
        "\n",
        "# Callbacks\n",
        "class EvalEveryNSteps(TrainerCallback):\n",
        "    def __init__(self, n=200): self.n=n\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s=int(state.global_step or 0)\n",
        "        if s and s%self.n==0:\n",
        "            control.should_log=True\n",
        "            control.should_evaluate=True\n",
        "\n",
        "class SentryMirror(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kw):\n",
        "        try:\n",
        "            out = Path(args.output_dir)\n",
        "            cks = [p for p in out.glob(\"checkpoint-*\") if p.is_dir()]\n",
        "            if not cks: return\n",
        "            ck = max(cks, key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "            has = (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "            if not has: return\n",
        "            dst = DRIVE/\"sentry_backups\"/RUN_ID/ck.name\n",
        "            if not dst.exists():\n",
        "                shutil.copytree(ck, dst)\n",
        "                print(f\"[SENTRY] {ck.name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[SENTRY] fail: {e}\")\n",
        "\n",
        "class EmbeddingSnap(TrainerCallback):\n",
        "    def __init__(self, every=50): self.every=every\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s=int(state.global_step or 0)\n",
        "        if s and s%self.every==0:\n",
        "            try:\n",
        "                E = model.get_input_embeddings().weight.detach().cpu()\n",
        "                out = (DRIVE/\"emb_snaps\"/RUN_ID)/f\"emb_step{s:04d}.pt\"\n",
        "                out.parent.mkdir(parents=True, exist_ok=True)\n",
        "                torch.save(E, out)\n",
        "                print(f\"[SNAP] {s}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[SNAP] fail: {e}\")\n",
        "\n",
        "class StepTimer(TrainerCallback):\n",
        "    def __init__(self, every=10):\n",
        "        self.prev=None\n",
        "        self.t=None\n",
        "        self.every=every\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s=int(state.global_step or 0)\n",
        "        now=time.time()\n",
        "        if self.prev and s>self.prev and s%self.every==0:\n",
        "            dt=now-self.t\n",
        "            print(f\"[{s:4d}] {dt/self.every:.1f}s/step\")\n",
        "        self.prev, self.t = s, now\n",
        "\n",
        "# Training config\n",
        "FINAL_TARGET = 1300\n",
        "LAST_STEP = 750\n",
        "TARGET = min(FINAL_TARGET, LAST_STEP + 550)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(LOCAL_RUN),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    max_steps=TARGET,\n",
        "    learning_rate=5e-4,\n",
        "    warmup_ratio=0.0,\n",
        "    optim=\"adafactor\",\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=75,\n",
        "    save_total_limit=2,  # Reduced\n",
        "    gradient_checkpointing=True,  # ENABLE\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    dataloader_num_workers=0,  # Changed\n",
        "    dataloader_pin_memory=False,  # Changed\n",
        "    dataloader_persistent_workers=False,  # Changed\n",
        "    report_to=[\"none\"],\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    data_collator=dc,\n",
        "    callbacks=[EvalEveryNSteps(200), SentryMirror(), EmbeddingSnap(50), StepTimer(10)],\n",
        ")\n",
        "\n",
        "# Ensure trainer state\n",
        "ts = RESUME_FROM/\"trainer_state.json\"\n",
        "if not ts.exists():\n",
        "    ts.write_text(json.dumps({\"global_step\":LAST_STEP, \"max_steps\":TARGET, \"log_history\":[]}, indent=2))\n",
        "    print(\"[RESUME] wrote state\")\n",
        "\n",
        "print(f\"[GO] 750→{TARGET} | MAX_LEN={MAX_LEN} | GC=ON\")\n",
        "t0 = time.time()\n",
        "trainer.train(resume_from_checkpoint=str(RESUME_FROM))\n",
        "print(f\"[DONE] {(time.time()-t0)/60:.1f}min\")"
      ],
      "metadata": {
        "id": "UrqDf71VEChl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FORCE CLEAN GPU MEMORY\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "for name in list(globals().keys()):\n",
        "    if 'model' in name.lower() or 'trainer' in name.lower():\n",
        "        try:\n",
        "            del globals()[name]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "print(\"[MEMORY] GPU cleaned\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAVXmnAcIH6s",
        "outputId": "ca2a3b53-6809-4dad-d792-4b20827ef421"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MEMORY] GPU cleaned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "for name in list(globals().keys()):\n",
        "    if 'model' in name.lower() or 'trainer' in name.lower():\n",
        "        try:\n",
        "            del globals()[name]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "print(\"[MEMORY] Cleaned\")"
      ],
      "metadata": {
        "id": "TDOSd5ARw7cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save directly to Drive\n",
        "import os, gc, torch, time, shutil\n",
        "from pathlib import Path\n",
        "from datasets import load_from_disk\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments,\n",
        "                          Trainer, TrainerCallback)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "RESUME_FROM = Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\")\n",
        "DRIVE = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "OUTPUT_DIR = DRIVE / \"runs\" / RUN_ID\n",
        "DATASETS = Path(\"/content/datasets\")\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(str(RESUME_FROM), use_fast=True)\n",
        "if tok.pad_token_id is None:\n",
        "    tok.pad_token = tok.eos_token or \"</s>\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    str(RESUME_FROM),\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=None,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "model.to(\"cuda\")\n",
        "model.config.use_cache = False\n",
        "model.config.attn_implementation = \"eager\"\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "emb = model.get_input_embeddings()\n",
        "emb.weight.requires_grad = True\n",
        "with torch.no_grad():\n",
        "    model.get_output_embeddings().weight = emb.weight\n",
        "model.train()\n",
        "\n",
        "print(\"[TIED]\", \"OK\")\n",
        "\n",
        "train_ds = load_from_disk(str(DATASETS/\"train_ds\"))\n",
        "valid_ds = load_from_disk(str(DATASETS/\"valid_ds\"))\n",
        "\n",
        "base_dc = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "MAX_LEN = 256\n",
        "\n",
        "class TruncatingCollator:\n",
        "    def __init__(self, base, max_len):\n",
        "        self.base, self.max_len = base, max_len\n",
        "    def __call__(self, feats):\n",
        "        out = self.base(feats)\n",
        "        for k,v in list(out.items()):\n",
        "            if isinstance(v, torch.Tensor) and v.dim()==2:\n",
        "                out[k] = v[:, :self.max_len]\n",
        "        return out\n",
        "\n",
        "dc = TruncatingCollator(base_dc, MAX_LEN)\n",
        "\n",
        "class StepTimer(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.prev=None\n",
        "        self.t=None\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        s=int(state.global_step or 0)\n",
        "        now=time.time()\n",
        "        if self.prev and s>self.prev and s%10==0:\n",
        "            print(f\"[{s:4d}] {(now-self.t)/10:.1f}s/step\")\n",
        "        self.prev, self.t = s, now\n",
        "\n",
        "TARGET = 1300\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    max_steps=TARGET,\n",
        "    learning_rate=5e-4,\n",
        "    warmup_ratio=0.0,\n",
        "    optim=\"adafactor\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=5,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    dataloader_num_workers=0,\n",
        "    dataloader_pin_memory=False,\n",
        "    report_to=[\"none\"],\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    data_collator=dc,\n",
        "    callbacks=[StepTimer()],\n",
        ")\n",
        "\n",
        "print(f\"[GO] 750→{TARGET} | DRIVE: {OUTPUT_DIR}\")\n",
        "t0 = time.time()\n",
        "trainer.train(resume_from_checkpoint=str(RESUME_FROM))\n",
        "print(f\"[DONE] {(time.time()-t0)/60:.1f}min\")"
      ],
      "metadata": {
        "id": "qZ2uCmfBw9iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CORRECTED - Save directly to Drive\n",
        "import os, gc, torch, time, shutil\n",
        "from pathlib import Path\n",
        "from datasets import load_from_disk\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments,\n",
        "                          Trainer, TrainerCallback)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "# CRITICAL FIX: Save directly to DRIVE\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "RESUME_FROM = Path(\"/content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-750-rebuilt\")\n",
        "DRIVE = Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "OUTPUT_DIR = DRIVE / \"runs\" / RUN_ID\n",
        "DATASETS = Path(\"/content/datasets\")\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "# Clean memory\n",
        "for name in list(globals().keys()):\n",
        "    if 'model' in name.lower() or 'trainer' in name.lower():\n",
        "        try:\n",
        "            del globals()[name]\n",
        "        except:\n",
        "            pass\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"[MEMORY] GPU cleaned\")\n",
        "\n",
        "# Load model (same as before)\n",
        "tok = AutoTokenizer.from_pretrained(str(RESUME_FROM), use_fast=True)\n",
        "if tok.pad_token_id is None:\n",
        "    tok.pad_token = tok.eos_token or \"</s>\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    str(RESUME_FROM),\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=None,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "model.to(\"cuda\")\n",
        "model.config.use_cache = False\n",
        "model.config.attn_implementation = \"eager\"\n",
        "\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "emb = model.get_input_embeddings()\n",
        "emb.weight.requires_grad = True\n",
        "with torch.no_grad():\n",
        "    model.get_output_embeddings().weight = emb.weight\n",
        "model.train()\n",
        "\n",
        "print(\"[TIED]\", \"OK\" if emb.weight.data_ptr()==model.get_output_embeddings().weight.data_ptr() else \"NO\")\n",
        "\n",
        "# Data\n",
        "train_ds = load_from_disk(str(DATASETS/\"train_ds\"))\n",
        "valid_ds = load_from_disk(str(DATASETS/\"valid_ds\"))\n",
        "print(f\"[DATA] train={len(train_ds)} valid={len(valid_ds)}\")\n",
        "\n",
        "base_dc = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "MAX_LEN = 256\n",
        "\n",
        "class TruncatingCollator:\n",
        "    def __init__(self, base, max_len):\n",
        "        self.base, self.max_len = base, max_len\n",
        "    def __call__(self, feats):\n",
        "        out = self.base(feats)\n",
        "        for k,v in list(out.items()):\n",
        "            if isinstance(v, torch.Tensor) and v.dim()==2:\n",
        "                out[k] = v[:, :self.max_len]\n",
        "        return out\n",
        "\n",
        "dc = TruncatingCollator(base_dc, MAX_LEN)\n",
        "\n",
        "# Minimal callbacks\n",
        "class StepTimer(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.prev=None\n",
        "        self.t=None\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        import time\n",
        "        s=int(state.global_step or 0)\n",
        "        now=time.time()\n",
        "        if self.prev and s>self.prev and s%10==0:\n",
        "            print(f\"[{s:4d}] {(now-self.t)/10:.1f}s/step\")\n",
        "        self.prev, self.t = s, now\n",
        "\n",
        "TARGET = 1300\n",
        "LAST_STEP = 750\n",
        "\n",
        "# CRITICAL: output_dir is now ON DRIVE\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(OUTPUT_DIR),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    max_steps=TARGET,\n",
        "    learning_rate=5e-4,\n",
        "    warmup_ratio=0.0,\n",
        "    optim=\"adafactor\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=5,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    dataloader_num_workers=0,\n",
        "    dataloader_pin_memory=False,\n",
        "    report_to=[\"none\"],\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    data_collator=dc,\n",
        "    callbacks=[StepTimer()],\n",
        ")\n",
        "\n",
        "print(f\"[GO] 750→{TARGET} | Saving to DRIVE: {OUTPUT_DIR}\")\n",
        "import time\n",
        "t0 = time.time()\n",
        "trainer.train(resume_from_checkpoint=str(RESUME_FROM))\n",
        "print(f\"[DONE] {(time.time()-t0)/60:.1f}min\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "CwsGPxibunds",
        "outputId": "b0628a4c-0dba-40f4-81d6-da7e6a025b83"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[MEMORY] GPU cleaned\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModelForCausalLM' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2887930348.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_token\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"</s>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRESUME_FROM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModelForCausalLM' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume P1 from best valid ckpt (300) → 750\n",
        "import pathlib, shutil, json, time, os, torch\n",
        "from datasets import load_from_disk\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          DataCollatorForLanguageModeling, TrainingArguments, Trainer, TrainerCallback)\n",
        "\n",
        "DRIVE = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUN_ID = max((DRIVE/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime).name\n",
        "DRIVE_RUN = DRIVE/\"runs\"/RUN_ID\n",
        "LOCAL_RUN = pathlib.Path(\"/content/runs\")/RUN_ID\n",
        "SENTRY    = DRIVE/\"sentry_backups\"/RUN_ID\n",
        "LOCAL_RUN.mkdir(parents=True, exist_ok=True); SENTRY.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def list_ckpts(root):\n",
        "    if not root.exists(): return []\n",
        "    return sorted([p for p in root.glob(\"checkpoint-*\") if p.is_dir()],\n",
        "                  key=lambda p: int(p.name.split(\"-\")[-1]), reverse=True)\n",
        "\n",
        "def has_weights(ck):\n",
        "    return (ck/\"model.safetensors\").exists() or (ck/\"pytorch_model.bin\").exists()\n",
        "\n",
        "# pick newest valid (weights) from runs or sentry (your table shows 300 is the last valid)\n",
        "cands = list_ckpts(DRIVE_RUN) + list_ckpts(SENTRY)\n",
        "GOOD = next((p for p in cands if has_weights(p)), None)\n",
        "assert GOOD is not None, \"No valid checkpoint with weights.\"\n",
        "last_step = int(GOOD.name.split(\"-\")[-1])\n",
        "print(f\"[RESUME] {RUN_ID} from {GOOD.name} (last_step={last_step})\")\n",
        "\n",
        "# seed local output_dir from that ckpt (fast, prevents partial Drive saves)\n",
        "if not (LOCAL_RUN/GOOD.name).exists():\n",
        "    shutil.copytree(GOOD, LOCAL_RUN/GOOD.name)\n",
        "    print(\"[LOCAL] seeded:\", (LOCAL_RUN/GOOD.name))\n",
        "\n",
        "# tokenizer + model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tok = AutoTokenizer.from_pretrained(str(GOOD), use_fast=True)\n",
        "if tok.pad_token_id is None: tok.pad_token = tok.eos_token or \"</s>\"\n",
        "model = AutoModelForCausalLM.from_pretrained(str(GOOD), torch_dtype=torch.float32, device_map=\"auto\")\n",
        "model.config.use_cache = False\n",
        "with torch.no_grad():\n",
        "    model.get_output_embeddings().weight = model.get_input_embeddings().weight  # tie head\n",
        "\n",
        "# datasets\n",
        "train_ds = load_from_disk(str(DRIVE/\"datasets\"/\"train_ds\"))\n",
        "valid_ds = load_from_disk(str(DRIVE/\"datasets\"/\"valid_ds\")).select(range(min(1000, len(load_from_disk(str(DRIVE/'datasets'/'valid_ds'))))))\n",
        "dc = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "\n",
        "TARGET = 1300\n",
        "\n",
        "# Callbacks\n",
        "from transformers import TrainerCallback\n",
        "\n",
        "class EvalEveryNSteps(TrainerCallback):\n",
        "    def __init__(self, n=200): self.n=n\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        if state.global_step and state.global_step % self.n == 0:\n",
        "            control.should_log = True\n",
        "            control.should_evaluate = True\n",
        "\n",
        "class SentryMirror(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kw):\n",
        "        try:\n",
        "            ck = max(LOCAL_RUN.glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]))\n",
        "            dst = SENTRY/ck.name\n",
        "            if not dst.exists():\n",
        "                shutil.copytree(ck, dst)\n",
        "                print(f\"[SENTRY] mirrored {ck.name}\")\n",
        "            # metrics\n",
        "            mdst = SENTRY/\"metrics\"; mdst.mkdir(parents=True, exist_ok=True)\n",
        "            msrc = LOCAL_RUN/\"metrics\"\n",
        "            if msrc.exists():\n",
        "                for f in msrc.glob(\"*.json\"): shutil.copy2(f, mdst/f.name)\n",
        "            os.sync()\n",
        "        except Exception as e:\n",
        "            print(\"[SENTRY] mirror failed:\", e)\n",
        "\n",
        "class EmbeddingSnap(TrainerCallback):\n",
        "    def __init__(self, every=50):\n",
        "        self.every = every\n",
        "        (DRIVE/\"emb_snaps\"/RUN_ID).mkdir(parents=True, exist_ok=True)\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        if state.global_step and state.global_step % self.every == 0:\n",
        "            try:\n",
        "                emb = model.get_input_embeddings().weight.detach().cpu()\n",
        "                path = DRIVE/\"emb_snaps\"/RUN_ID/f\"emb_step{int(state.global_step):04d}.pt\"\n",
        "                torch.save(emb, path)\n",
        "                (DRIVE/\"emb_snaps\"/RUN_ID/\"heartbeat.json\").write_text(json.dumps(\n",
        "                    {\"step\": int(state.global_step), \"rows\": int(emb.size(0)), \"dim\": int(emb.size(1)), \"ts\": time.time()}, indent=2))\n",
        "                print(f\"[SNAP] embeddings → {path.name}\")\n",
        "            except Exception as e:\n",
        "                print(\"[SNAP] failed:\", e)\n",
        "\n",
        "# freq at 50 until 700, then 100\n",
        "save_steps = 50 if last_step < 700 else 100\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(LOCAL_RUN),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    max_steps=TARGET,\n",
        "    learning_rate=5e-4,\n",
        "    warmup_ratio=0.0,\n",
        "    optim=\"adafactor\",\n",
        "    logging_steps=50,\n",
        "    save_strategy=\"steps\", save_steps=save_steps, save_total_limit=20,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False, bf16=False,\n",
        "    report_to=[\"none\"],\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    data_collator=dc,\n",
        "    callbacks=[EvalEveryNSteps(200), SentryMirror(), EmbeddingSnap(50)],\n",
        ")\n",
        "\n",
        "trainer.train(resume_from_checkpoint=str(LOCAL_RUN/GOOD.name))\n",
        "# Finalize\n",
        "trainer.save_model(str(LOCAL_RUN/\"checkpoint-final\")); tok.save_pretrained(str(LOCAL_RUN/\"checkpoint-final\"))\n",
        "# Mirror final\n",
        "dst = SENTRY/\"checkpoint-final\"\n",
        "if dst.exists(): shutil.rmtree(dst)\n",
        "shutil.copytree(LOCAL_RUN/\"checkpoint-final\", dst)\n",
        "print(\"[SENTRY] mirrored checkpoint-final\")\n",
        "print(\"[DONE] Reached\", TARGET)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "tLSo5SLZGpLR",
        "outputId": "16aede5c-a9f0-4350-852f-3c8540b57d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RESUME] t4_1762376560 from checkpoint-300 (last_step=300)\n",
            "[LOCAL] seeded: /content/runs/t4_1762376560/checkpoint-300\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /content/drive/MyDrive/wake2vec/runs/t4_1762376560/checkpoint-300 and are newly initialized: ['lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n",
            "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
            "\tsave_steps: 50 (from args) != 100 (from trainer_state.json)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='767' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 767/1300 3:03:39 < 3:30:31, 0.04 it/s, Epoch 13.44/23]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>5.488300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>5.308200</td>\n",
              "      <td>6.273335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>4.777600</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.089100</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.260400</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.425900</td>\n",
              "      <td>7.170007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.554100</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.798300</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.319200</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SNAP] embeddings → emb_step0350.pt\n",
            "[SNAP] embeddings → emb_step0400.pt\n",
            "[SNAP] embeddings → emb_step0450.pt\n",
            "[SNAP] embeddings → emb_step0500.pt\n",
            "[SNAP] embeddings → emb_step0550.pt\n",
            "[SNAP] embeddings → emb_step0600.pt\n",
            "[SNAP] embeddings → emb_step0650.pt\n",
            "[SNAP] embeddings → emb_step0700.pt\n",
            "[SNAP] embeddings → emb_step0750.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"CUDA:\", torch.cuda.is_available(), \"| device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyEsFTWMF_mQ",
        "outputId": "cfa2420b-08df-43a8-805c-3ddede0decda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA: True | device: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pin + shim\n",
        "import sys, subprocess, importlib, os\n",
        "\n",
        "def pin(pkg, ver):\n",
        "    try:\n",
        "        m = importlib.import_module(pkg)\n",
        "        assert m.__version__ == ver\n",
        "        print(f\"[OK] {pkg} {m.__version__}\")\n",
        "    except Exception:\n",
        "        print(f\"[PIN] {pkg}=={ver}\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"{pkg}=={ver}\", \"-q\"])\n",
        "        m = importlib.import_module(pkg)\n",
        "        print(f\"[OK] {pkg} {m.__version__}\")\n",
        "\n",
        "pin(\"transformers\", \"4.57.1\")\n",
        "pin(\"accelerate\",   \"1.2.1\")\n",
        "pin(\"datasets\",     \"2.21.0\")\n",
        "\n",
        "# unwrap_model shim (ignore keep_torch_compile kw)\n",
        "import accelerate\n",
        "if not hasattr(accelerate.Accelerator, \"_w2v_patched\"):\n",
        "    _orig = accelerate.Accelerator.unwrap_model\n",
        "    def _shim(self, model, *args, **kw):\n",
        "        kw.pop(\"keep_torch_compile\", None)\n",
        "        return _orig(self, model, *args, **kw)\n",
        "    accelerate.Accelerator.unwrap_model = _shim\n",
        "    accelerate.Accelerator._w2v_patched = True\n",
        "    print(\"[PATCH] unwrap_model shim active\")\n",
        "\n",
        "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "1q3lzL0pF2X0",
        "outputId": "2991c94e-1854-4bdf-aa5c-bb75914eaa99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] transformers 4.57.1\n",
            "[PIN] accelerate==1.2.1\n",
            "[OK] accelerate 1.11.0\n",
            "[PIN] datasets==2.21.0\n",
            "[OK] datasets 4.0.0\n",
            "[PATCH] unwrap_model shim active\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'expandable_segments:True'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Force resume from rebuilt 750\n",
        "RUN_ID = \"t4_1762376560\"\n",
        "RESUME_FROM = f\"/content/drive/MyDrive/wake2vec/runs/{RUN_ID}/checkpoint-750-rebuilt\"\n",
        "print(\"[RESUME FROM]\", RESUME_FROM)\n",
        "\n",
        "FINAL_TARGET = 1300\n",
        "last_step = 750\n",
        "TARGET = min(FINAL_TARGET, last_step + 300)\n",
        "save_steps = 75 if last_step < 1000 else 100\n",
        "print(f\"[PLAN] last={last_step} → target={TARGET} | save_steps={save_steps}\")\n",
        "\n",
        "RESUME_FROM = f\"/content/drive/MyDrive/wake2vec/runs/{RUN_ID}/checkpoint-750-rebuilt\"\n",
        "print(\"[RESUME FROM]\", RESUME_FROM)"
      ],
      "metadata": {
        "id": "npp7gTpm8GdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for p1 eval"
      ],
      "metadata": {
        "id": "jRvQ2ZhVQ389"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TRAIN output_dir:\", trainer.args.output_dir)\n",
        "print(\"RUN_ID:\", pathlib.Path(trainer.args.output_dir).name)\n",
        "print(\"global_step:\", trainer.state.global_step)"
      ],
      "metadata": {
        "id": "4Q8WwTrs-zCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "from transformers import TrainingArguments\n",
        "'evaluation_strategy' in inspect.signature(TrainingArguments.__init__).parameters"
      ],
      "metadata": {
        "id": "eC9-VVNQ2KC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class EvalEveryNSteps(TrainerCallback):\n",
        "    def __init__(self, n=200): self.n=n\n",
        "    def on_step_end(self, args, state, control, **kw):\n",
        "        if state.global_step and (state.global_step % self.n == 0):\n",
        "            control.should_save = True         # ensure state gets flushed\n",
        "            control.should_log = True\n",
        "            control.should_evaluate = True"
      ],
      "metadata": {
        "id": "yX9eVZvc2SCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# P1 finalise overlap@5, norm stats, and a loss plot\n",
        "import json, numpy as np, pathlib, matplotlib.pyplot as plt\n",
        "\n",
        "DRIVE_ROOT = pathlib.Path(\"/content/drive/MyDrive/wake2vec\")\n",
        "RUNS = sorted((DRIVE_ROOT/\"runs\").glob(\"t4_*\"), key=lambda p: p.stat().st_mtime)\n",
        "RUN_DIR = pathlib.Path(\"/content/runs\")/RUNS[-1].name\n",
        "METRICS_DIR = RUN_DIR/\"metrics\"\n",
        "PLOTS_DIR = RUN_DIR/\"plots\"; PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Load current embeddings\n",
        "from transformers import AutoModelForCausalLM\n",
        "BASE_CKPT = RUN_DIR/\"checkpoint-final\"\n",
        "model = AutoModelForCausalLM.from_pretrained(str(BASE_CKPT), torch_dtype=\"float32\", device_map=None)\n",
        "E_post = model.get_input_embeddings().weight.detach().cpu().numpy()\n",
        "\n",
        "# Optional composed init + ids\n",
        "E_COMP = DRIVE_ROOT/\"E_comp.npy\"\n",
        "NEW_IDS = DRIVE_ROOT/\"new_ids.npy\"\n",
        "has_comp = E_COMP.exists() and NEW_IDS.exists()\n",
        "\n",
        "def topk_overlap(a, b, k=5):\n",
        "    import numpy as np\n",
        "    from numpy.linalg import norm\n",
        "    a = a / (norm(a, axis=1, keepdims=True)+1e-9)\n",
        "    b = b / (norm(b, axis=1, keepdims=True)+1e-9)\n",
        "    sims = a @ b.T\n",
        "    top_a = np.argsort(-sims, axis=1)[:, :k]\n",
        "    top_b = np.argsort(-sims, axis=1)[:, :k]\n",
        "    inter = np.array([len(set(top_a[i]) & set(top_b[i])) for i in range(a.shape[0])])\n",
        "    return inter.mean()\n",
        "\n",
        "report = {}\n",
        "\n",
        "if has_comp:\n",
        "    E_comp = np.load(E_COMP)\n",
        "    new_ids = np.load(NEW_IDS)\n",
        "    E_post_new = E_post[new_ids]\n",
        "    overlap5 = topk_overlap(E_comp, E_post_new, k=5)\n",
        "    # Norm drift\n",
        "    from numpy.linalg import norm\n",
        "    dn = (norm(E_post_new, axis=1) - norm(E_comp, axis=1)).mean()\n",
        "    report.update({\"overlap_at_5\": float(overlap5), \"mean_delta_norm\": float(dn), \"n_new\": int(len(new_ids))})\n",
        "else:\n",
        "    # Fallback\n",
        "    from numpy.linalg import norm\n",
        "    norms = norm(E_post, axis=1)\n",
        "    report.update({\"post_mean_norm\": float(norms.mean()), \"post_std_norm\": float(norms.std()), \"n_vocab\": int(E_post.shape[0])})\n",
        "\n",
        "# JSON\n",
        "(METRICS_DIR/\"p1_summary.json\").write_text(json.dumps(report, indent=2))\n",
        "print(\"[P1 SUMMARY]\", json.dumps(report, indent=2))\n",
        "\n",
        "# Loss plot\n",
        "import json, glob\n",
        "state_files = [RUN_DIR/\"trainer_state.json\", BASE_CKPT/\"trainer_state.json\"]\n",
        "state_files = [p for p in state_files if p.exists()]\n",
        "logs = []\n",
        "for sf in state_files:\n",
        "    s = json.loads(sf.read_text())\n",
        "    logs.extend([d for d in s.get(\"log_history\", []) if \"loss\" in d])\n",
        "\n",
        "if logs:\n",
        "    steps = [d[\"step\"] for d in logs]\n",
        "    losses = [float(d[\"loss\"]) for d in logs]\n",
        "    ema = []\n",
        "    alpha = 0.1\n",
        "    for i,x in enumerate(losses):\n",
        "        ema.append(x if i==0 else alpha*x + (1-alpha)*ema[-1])\n",
        "    plt.figure(figsize=(7,4.5))\n",
        "    plt.plot(steps, losses, label=\"loss\")\n",
        "    plt.plot(steps, ema, label=\"EMA(0.1)\")\n",
        "    plt.title(f\"P1 Loss — {RUN_DIR.name}\")\n",
        "    plt.xlabel(\"step\"); plt.ylabel(\"loss\"); plt.grid(True, linewidth=0.3); plt.legend()\n",
        "    outp = PLOTS_DIR/\"p1_loss_curve.png\"\n",
        "    plt.savefig(outp, dpi=140, bbox_inches=\"tight\")\n",
        "    print(\"[PLOT]\", outp)\n",
        "else:\n",
        "    print(\"[WARN] No trainer_state logs found; skip loss plot.\")"
      ],
      "metadata": {
        "id": "5AMneAlxqpmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Config and Execution\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Training args\n",
        "args = TrainingArguments(\n",
        "    output_dir=str(LOCAL_RUN),\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    max_steps=TARGET,\n",
        "    learning_rate=5e-4,\n",
        "    warmup_ratio=0.0,\n",
        "    optim=\"adafactor\",\n",
        "    logging_steps=25,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=75,\n",
        "    save_total_limit=3,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    dataloader_num_workers=0,\n",
        "    dataloader_pin_memory=False,\n",
        "    dataloader_persistent_workers=False,\n",
        "    report_to=[\"none\"],\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=valid_ds,\n",
        "    data_collator=dc_trunc,\n",
        "    callbacks=[\n",
        "        EvalEveryNSteps(200),\n",
        "        SentryMirror(),\n",
        "        EmbeddingSnap(50),\n",
        "        StepTimer(10)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# resume state\n",
        "ts = RESUME_FROM / \"trainer_state.json\"\n",
        "if not ts.exists():\n",
        "    state_dict = {\n",
        "        \"global_step\": LAST_STEP,\n",
        "        \"max_steps\": TARGET,\n",
        "        \"log_history\": []\n",
        "    }\n",
        "    ts.write_text(json.dumps(state_dict, indent=2))\n",
        "    print(f\"[RESUME] wrote trainer_state.json at step {LAST_STEP}\")\n",
        "\n",
        "# training\n",
        "print(f\"[GO] Resuming from step {LAST_STEP} with MAX_LEN={MAX_LEN}\")\n",
        "print(f\"     Target: {TARGET} | Gradient checkpointing: ON\")\n",
        "\n",
        "t0 = time.time()\n",
        "trainer.train(resume_from_checkpoint=str(RESUME_FROM))\n",
        "elapsed = time.time() - t0\n",
        "\n",
        "print(f\"[COMPLETE] Training finished in {elapsed:.1f}s ({elapsed/60:.1f}min)\")"
      ],
      "metadata": {
        "id": "etUEGZNd7Qvl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}